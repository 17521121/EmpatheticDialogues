{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "S_dusNBSwyQA"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTpitzzYvYQQ",
        "colab_type": "text"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4enR4ml-vcGJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "9945a1cd-3543-4031-cf85-a395eb097fde"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK7IHJoYvcj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install fairseq\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4abH6L1vtH7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64fecd47-0928-43cc-bcfa-34229f85576c"
      },
      "source": [
        "%cd drive/My Drive/Colab Notebooks/chatbot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/chatbot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9q2YiXSv8wf",
        "colab_type": "text"
      },
      "source": [
        "# Body"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eafuPRdnWNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bertmodel_name = \"bert-base-multilingual-cased\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjnGKvEowNEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import argparse\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogaLueB9mAzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig\n",
        "from transformers import BertModel\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, device='cpu', pretrain = True):\n",
        "        super().__init__()\n",
        "        config = BertConfig.from_pretrained(bertmodel_name)\n",
        "        if pretrain:\n",
        "            # roberta just a name, should be self.bert\n",
        "            self.roberta = BertModel.from_pretrained(bertmodel_name)\n",
        "        else:\n",
        "            self.roberta = BertModel(config)\n",
        "\n",
        "        self.fc = nn.Linear(768, 300)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.device == \"cuda\":\n",
        "            x = x.to(self.device, non_blocking=True)\n",
        "        else:\n",
        "            x = x.to(self.device)\n",
        "        \n",
        "        if self.training:\n",
        "            self.roberta.train()\n",
        "            enc, _ = self.roberta(x)\n",
        "        else:\n",
        "            self.roberta.eval()\n",
        "            with torch.no_grad():\n",
        "                enc, _ = self.roberta(x)\n",
        "        \n",
        "        # CLS\n",
        "        enc = enc[:, 0, :]\n",
        "\n",
        "        result  = self.fc(enc)\n",
        "        return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUgGs4w5mttS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils import data\n",
        " \n",
        "from fairseq.data import Dictionary\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(bertmodel_name)\n",
        "\n",
        "# tokenizer.add_tokens(\"[SOC]\")\n",
        "pad_idx = tokenizer.pad_token_id\n",
        " \n",
        "def preprocess(text, split_sep = \" \"):\n",
        "    return (\n",
        "        text.replace(\".\", \" . \")\n",
        "        .replace(\"_comma_\", \",\")\n",
        "        .replace(\". . .\", \"...\")\n",
        "        .replace(\",\", \" , \")\n",
        "        .replace(\";\", \" ; \")\n",
        "        .replace(\":\", \" : \")\n",
        "        .replace(\"!\", \" ! \")\n",
        "        .replace(\"'\", \" ' \")\n",
        "        .replace(\"?\", \" ? \")\n",
        "        .replace(\"  \", \" \")\n",
        "        .replace(\"  \", \" \")\n",
        "        .strip()\n",
        "        .lower() # adjust\n",
        "        # .split(split_sep)\n",
        "    )\n",
        "\n",
        "\n",
        "def txt2vec(text, max_tokens_len):\n",
        "   \n",
        "    text = preprocess(text)\n",
        "     \n",
        "    subwords = '[CLS] ' + text + ' [SEP]'\n",
        "    #text to tensor\n",
        "    input_ids = torch.tensor(tokenizer.encode(subwords, add_special_tokens=False)).long().tolist()[:max_tokens_len]\n",
        "    return torch.LongTensor([input_ids])\n",
        "\n",
        "def batchify(batch):\n",
        "        \n",
        "        input_list = list(zip(*batch))\n",
        "        contexts, next_ = [\n",
        "            pad(ex, pad_idx) for ex in [input_list[0], input_list[1]]\n",
        "        ]\n",
        "        \n",
        "        return contexts, next_, input_list[2]\n",
        "\n",
        "def pad(tensors, padding_value=-1, max_len = 256):\n",
        "    \"\"\"\n",
        "    Concatenate and pad the input tensors, which may be 1D or 2D.\n",
        "    \"\"\"\n",
        "\n",
        "    max_len = max(t.size(-1) for t in tensors) \n",
        "\n",
        "    if tensors[0].dim() == 1:\n",
        "        out = torch.LongTensor(len(tensors), max_len).fill_(padding_value)\n",
        "        for i, t in enumerate(tensors):\n",
        "            out[i, : t.size(0)] = t\n",
        "        return out\n",
        "    elif tensors[0].dim() == 2:\n",
        "        max_width = max(t.size(0) for t in tensors)\n",
        "        out = torch.LongTensor(len(tensors), max_width, max_len).fill_(padding_value)\n",
        "        for i, t in enumerate(tensors):\n",
        "            out[i, : t.size(0), : t.size(1)] = t\n",
        "        return out\n",
        "    else:\n",
        "        raise ValueError(\"Input tensors must be either 1D or 2D!\")\n",
        "\n",
        "\n",
        "\n",
        "class EmpDataset(data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        splitname,\n",
        "        maxlen=256, # max number of tokens per sentence # chua su dung\n",
        "        history_len=3,\n",
        "    ):\n",
        "        df = open(f\"{splitname}.csv\", \"r\" ,encoding=\"utf-8\").readlines()\n",
        "         \n",
        "        self.max_hist_len = history_len\n",
        "        self.data = []\n",
        "        self.ids = []\n",
        "        history = []\n",
        "        for i in range(1, len(df)):\n",
        "            cparts = df[i - 1].strip().split(\",\")\n",
        "            sparts = df[i].strip().split(\",\")\n",
        "            if cparts[0] == sparts[0]:\n",
        "\n",
        "                history.append(cparts[5])\n",
        "                \n",
        "                idx = int(sparts[1])\n",
        "                if (idx % 2) == 0:\n",
        "                    \n",
        "                    # SOC|SEP start of comment\n",
        "                    sentence1 = \" [SEP] \".join(history[-self.max_hist_len :]) \n",
        "                    sentence2 =  sparts[5]\n",
        "\n",
        "                    self.data.append((txt2vec(sentence1, maxlen), txt2vec(sentence2, maxlen), sparts[2]))\n",
        "                    self.ids.append((sparts[0], sparts[1]))\n",
        "                    \n",
        "            else:\n",
        "                history = []\n",
        "                    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "        \n",
        "    def getid(self, index):\n",
        "        return self.ids[index]\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma0IfG7MwRLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        " \n",
        "max_tokens_length = 256\n",
        " \n",
        "class Args():\n",
        "  def __init__(self):\n",
        "    self.optimizer = \"adamax\"\n",
        "    self.learning_rate = 1e-5\n",
        "    self.stop_crit_num_epochs = 10\n",
        "    self.batch_size = 20\n",
        "    self.max_turn = 4\n",
        "    self.epochs = 30\n",
        "    self.hits_at_nb_cands = 100 # p@1,100\n",
        "    self.display_iter = 100 # help=\"Frequency of train logging\"\n",
        "    self.log_file = \"logs/bert.txt\"\n",
        "    self.model_file = \"models/bert.pt\"\n",
        "\n",
        "option = Args()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPpOEHfBwT5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = EmpDataset(\n",
        "  \"ED/train\",\n",
        "  maxlen = max_tokens_length,\n",
        "  history_len = option.max_turn,\n",
        ")\n",
        "\n",
        "dev_dataset = EmpDataset(\n",
        "  \"ED/valid\",\n",
        "  maxlen = max_tokens_length,\n",
        "  history_len = option.max_turn,\n",
        ")\n",
        "\n",
        "train_iter = DataLoader(\n",
        "  dataset     = train_dataset,\n",
        "  batch_size  = option.batch_size,\n",
        "  shuffle     = True,\n",
        "  num_workers = 0,\n",
        "  collate_fn  = batchify,\n",
        "  pin_memory  = True,\n",
        ")\n",
        "\n",
        "\n",
        "dev_iter = DataLoader(\n",
        "  dataset     = dev_dataset,\n",
        "  batch_size  = option.batch_size,\n",
        "  shuffle     = True,\n",
        "  num_workers = 0,\n",
        "  collate_fn  = batchify,\n",
        "  pin_memory  = True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm1Kwo1QwUg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(train_iter,\"torch_pre_load/bert_train.pth\")\n",
        "torch.save(dev_iter,\"torch_pre_load/bert_dev.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bmj9f5EwWB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_iter = torch.load(\"torch_pre_load/train_auto.pth\")\n",
        "# dev_iter = torch.load(\"torch_pre_load/dev_auto.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG7babAtwXoV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "c1aa5354-aec3-4928-c758-5834013c26de"
      },
      "source": [
        "import logging\n",
        "import sys\n",
        "import json\n",
        "\n",
        "def get_logger(opt):\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.INFO)\n",
        "    fmt = logging.Formatter(\"%(asctime)s: [ %(message)s ]\", \"%m/%d/%Y %I:%M:%S %p\")\n",
        "    console = logging.StreamHandler()\n",
        "    console.setFormatter(fmt)\n",
        "    logger.handlers = []\n",
        "    logger.addHandler(console)\n",
        "    if opt.log_file:\n",
        "        logfile = logging.FileHandler(opt.log_file, \"a\")\n",
        "        logfile.setFormatter(fmt)\n",
        "        logger.addHandler(logfile)\n",
        "    command = \" \".join(sys.argv)\n",
        "    # logger.info(f\"COMMAND: {command}\")\n",
        "    # logger.info(\"-\" * 100)\n",
        "    config = json.dumps(vars(opt), indent=4, sort_keys=True)\n",
        "    logger.info(f\"CONFIG:\\n{config}\")\n",
        "    return logger\n",
        "logger = get_logger(option)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "09/08/2020 04:19:13 AM: [ CONFIG:\n",
            "{\n",
            "    \"batch_size\": 20,\n",
            "    \"display_iter\": 100,\n",
            "    \"epochs\": 30,\n",
            "    \"hits_at_nb_cands\": 100,\n",
            "    \"learning_rate\": 1e-05,\n",
            "    \"log_file\": \"logs/bert_auto.txt\",\n",
            "    \"max_turn\": 4,\n",
            "    \"model_file\": \"models/bert_auto.pt\",\n",
            "    \"optimizer\": \"adamax\",\n",
            "    \"stop_crit_num_epochs\": 10\n",
            "} ]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYWqC-rtwZWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score_candidates(all_context, all_cands, top_k=20, normalize=False):\n",
        "    \n",
        "    dot_products = all_context.mm(all_cands.t())  # [ctx, cand]\n",
        "    if normalize:\n",
        "        dot_products /= all_context.norm(2, dim=1).unsqueeze(1)\n",
        "        dot_products /= all_cands.norm(2, dim=1).unsqueeze(0)\n",
        "    scores, answers = dot_products.topk(top_k, dim=1)\n",
        "    # Index of top-k items in decreasing order. Answers is of size [ctx, top_k]\n",
        "    return scores, answers\n",
        "\n",
        "def loss_fn(ctx, labels):\n",
        "     \n",
        "    batch_size = ctx.size(0)\n",
        "    dot_products = ctx.mm(labels.t())\n",
        "   \n",
        "    log_prob = F.log_softmax(dot_products, dim=1)\n",
        "    targets = log_prob.new_empty(batch_size).long()\n",
        "    targets = torch.arange(batch_size, out=targets)\n",
        "    loss = F.nll_loss(log_prob, targets)\n",
        "    nb_ok = (log_prob.max(dim=1)[1] == targets).float().sum()\n",
        "    return loss, nb_ok\n",
        "\n",
        "def train(epoch, start_time, model, optimizer, opt, data_loader):\n",
        "    \"\"\"Run through one epoch of model training with the provided data loader.\"\"\"\n",
        "    model.train()\n",
        "    # Initialize meters + timers\n",
        "    train_loss = 0\n",
        "    nb_ok = 0\n",
        "    nb_exs = 0\n",
        "    nb_losses = 0\n",
        "    epoch_start = time.time()\n",
        "    # Run one epoch\n",
        "    for idx, ex in enumerate(data_loader, 1):\n",
        "        params = [\n",
        "            field\n",
        "            if field is not None\n",
        "            else None\n",
        "            for field in ex\n",
        "        ]\n",
        "        ctx = model(params[0][:,0,:])\n",
        "        cands = model(params[1][:,0,:])\n",
        "        loss, ok = loss_fn(ctx, cands)\n",
        "        nb_ok += ok\n",
        "        nb_exs += ex[0].size(0)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.sum().item()\n",
        "        nb_losses += 1\n",
        "        if idx % opt.display_iter == 0 or idx == len(data_loader):\n",
        "            avg_loss = train_loss / nb_losses\n",
        "            acc = 100 * nb_ok / nb_exs\n",
        "            elapsed = time.time() - start_time\n",
        "            logging.info(\n",
        "                f\"train: Epoch = {epoch} | iter = {idx}/{len(data_loader)} | loss = \"\n",
        "                f\"{avg_loss:.3f} | batch P@1 = {acc:.2f} % | elapsed time = \"\n",
        "                f\"{elapsed:.2f} (s)\"\n",
        "            )\n",
        "            train_loss = 0\n",
        "            nb_losses = 0\n",
        "    epoch_elapsed = time.time() - epoch_start\n",
        "    logging.info(\n",
        "        f\"train: Epoch {epoch:d} done. Time for epoch = {epoch_elapsed:.2f} (s)\"\n",
        "    )\n",
        "\n",
        "\n",
        "def validate(\n",
        "    epoch,\n",
        "    model,\n",
        "    data_loader,   \n",
        "    is_test=False,\n",
        "    nb_candidates=100,\n",
        "    shuffle=\"shuffled\",\n",
        "):\n",
        "    model.eval()\n",
        "    examples = 0\n",
        "    eval_start = time.time()\n",
        "    sum_losses = 0\n",
        "    n_losses = 0\n",
        "    correct = 0\n",
        "    all_context = []\n",
        "    all_cands = []\n",
        "    n_skipped = 0\n",
        "   \n",
        "    for i, ex in enumerate(data_loader):\n",
        "        batch_size = ex[0].size(0)\n",
        "        \n",
        "        params = [\n",
        "            field\n",
        "            if field is not None\n",
        "            else None\n",
        "            for field in ex\n",
        "        ]\n",
        "        # ctx, cands = model(*params)\n",
        "        ctx = model(params[0][:,0,:])\n",
        "        cands = model(params[1][:,0,:])\n",
        "        all_context.append(ctx)\n",
        "        all_cands.append(cands)\n",
        "        loss, nb_ok = loss_fn(ctx, cands)\n",
        "        sum_losses += loss\n",
        "        correct += nb_ok\n",
        "        n_losses += 1\n",
        "        examples += batch_size\n",
        "        \n",
        "    n_examples = 0\n",
        "    if len(all_context) > 0:\n",
        "        logging.info(\"Processing candidate top-K\")\n",
        "        all_context = torch.cat(all_context, dim=0)  # [:50000]  # [N, 2h]\n",
        "        all_cands = torch.cat(all_cands, dim=0)  # [:50000]  # [N, 2h]\n",
        "        acc_ranges = [1, 3, 10]\n",
        "        n_correct = {r: 0 for r in acc_ranges}\n",
        "        for context, cands in list(\n",
        "            zip(all_context.split(nb_candidates), all_cands.split(nb_candidates))\n",
        "        )[:-1]:\n",
        "            _, top_answers = score_candidates(context, cands)\n",
        "            n_cands = cands.size(0)\n",
        "            gt_index = torch.arange(n_cands, out=top_answers.new(n_cands, 1))\n",
        "            for acc_range in acc_ranges:\n",
        "                n_acc = (top_answers[:, :acc_range] == gt_index).float().sum()\n",
        "                n_correct[acc_range] += n_acc\n",
        "            n_examples += n_cands\n",
        "        accuracies = {r: 100 * n_acc / n_examples for r, n_acc in n_correct.items()}\n",
        "        avg_loss = sum_losses / (n_losses + 0.00001)\n",
        "        avg_acc = 100 * correct / (examples + 0.000001)\n",
        "        valid_time = time.time() - eval_start\n",
        "        logging.info(\n",
        "            f\"Valid ({shuffle}): Epoch = {epoch:d} | avg loss = {avg_loss:.3f} | \"\n",
        "            f\"batch P@1 = {avg_acc:.2f} % | \"\n",
        "            + f\" | \".join(\n",
        "                f\"P@{k},{nb_candidates} = {v:.2f}%\" for k, v in accuracies.items()\n",
        "            )\n",
        "            + f\" | valid time = {valid_time:.2f} (s)\"\n",
        "        )\n",
        "        return avg_loss\n",
        "    return 10\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9II3kAYwjFB",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPVBll9lwk0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Net(device)\n",
        "\n",
        "if device == \"cuda\":\n",
        "  torch.cuda.set_device(-1) # get the lastest device (GPU)\n",
        "  net = torch.nn.DataParallel(net)\n",
        "  net.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXg5BYXj8HPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Net(device, False)\n",
        "\n",
        "if device == \"cuda\":\n",
        "  torch.cuda.set_device(-1) # get the lastest device (GPU)\n",
        "  net = torch.nn.DataParallel(net)\n",
        "  net.cuda()\n",
        "  \n",
        "# ctx_net.to(device)\n",
        "net.load_state_dict(torch.load(option.model_file), strict = False)\n",
        "net.eval()\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iR_xqf1v-oJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21b09333-2b2e-421a-a234-c80d63bbb2c5"
      },
      "source": [
        "###############\n",
        "### Train #####\n",
        "###############\n",
        "    \n",
        "if option.optimizer == \"adamax\":\n",
        "    lr = option.learning_rate\n",
        "    named_params_to_optimize = filter(\n",
        "        lambda p: p[1].requires_grad, net.named_parameters()\n",
        "    )\n",
        "    params_to_optimize = (p[1] for p in named_params_to_optimize)\n",
        "    optimizer = optim.Adamax(params_to_optimize, lr=lr)\n",
        "\n",
        "    \n",
        "start_time = time.time()\n",
        "best_loss = float(\"+inf\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    validate(\n",
        "        0,\n",
        "        net,\n",
        "        dev_iter,\n",
        "        shuffle = False,\n",
        "        nb_candidates=option.hits_at_nb_cands,\n",
        "    )\n",
        "\n",
        " \n",
        "for epoch in range(0, option.epochs):\n",
        "    train(epoch, start_time, net, optimizer, option, train_iter)\n",
        "    with torch.no_grad():\n",
        "        loss = validate(\n",
        "            epoch,\n",
        "            net,\n",
        "            dev_iter,\n",
        "            nb_candidates=option.hits_at_nb_cands,\n",
        "        )\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_loss_epoch = epoch\n",
        "            logging.info(f\"New best loss, saving model to {option.model_file}\")\n",
        "            torch.save(net.state_dict(), f\"{option.model_file}\")\n",
        "        # Stop if it's been too many epochs since the loss has decreased\n",
        "        if option.stop_crit_num_epochs != -1:\n",
        "            if epoch - best_loss_epoch >= option.stop_crit_num_epochs:\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "09/08/2020 04:20:01 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 04:20:01 AM: [ Valid (False): Epoch = 0 | avg loss = 1.505 | batch P@1 = 54.91 % | P@1,100 = 33.65% | P@3,100 = 51.98% | P@10,100 = 72.47% | valid time = 29.49 (s) ]\n",
            "09/08/2020 04:20:35 AM: [ train: Epoch = 0 | iter = 100/2012 | loss = 1.521 | batch P@1 = 54.50 % | elapsed time = 64.07 (s) ]\n",
            "09/08/2020 04:21:10 AM: [ train: Epoch = 0 | iter = 200/2012 | loss = 1.518 | batch P@1 = 54.53 % | elapsed time = 98.48 (s) ]\n",
            "09/08/2020 04:21:46 AM: [ train: Epoch = 0 | iter = 300/2012 | loss = 1.482 | batch P@1 = 54.05 % | elapsed time = 134.67 (s) ]\n",
            "09/08/2020 04:22:21 AM: [ train: Epoch = 0 | iter = 400/2012 | loss = 1.470 | batch P@1 = 54.10 % | elapsed time = 170.03 (s) ]\n",
            "09/08/2020 04:22:56 AM: [ train: Epoch = 0 | iter = 500/2012 | loss = 1.539 | batch P@1 = 53.87 % | elapsed time = 205.24 (s) ]\n",
            "09/08/2020 04:23:32 AM: [ train: Epoch = 0 | iter = 600/2012 | loss = 1.545 | batch P@1 = 53.66 % | elapsed time = 241.25 (s) ]\n",
            "09/08/2020 04:24:09 AM: [ train: Epoch = 0 | iter = 700/2012 | loss = 1.449 | batch P@1 = 53.91 % | elapsed time = 277.56 (s) ]\n",
            "09/08/2020 04:24:45 AM: [ train: Epoch = 0 | iter = 800/2012 | loss = 1.484 | batch P@1 = 54.02 % | elapsed time = 313.83 (s) ]\n",
            "09/08/2020 04:25:20 AM: [ train: Epoch = 0 | iter = 900/2012 | loss = 1.489 | batch P@1 = 54.12 % | elapsed time = 348.66 (s) ]\n",
            "09/08/2020 04:25:55 AM: [ train: Epoch = 0 | iter = 1000/2012 | loss = 1.505 | batch P@1 = 54.08 % | elapsed time = 383.69 (s) ]\n",
            "09/08/2020 04:26:31 AM: [ train: Epoch = 0 | iter = 1100/2012 | loss = 1.524 | batch P@1 = 53.96 % | elapsed time = 419.91 (s) ]\n",
            "09/08/2020 04:27:06 AM: [ train: Epoch = 0 | iter = 1200/2012 | loss = 1.467 | batch P@1 = 53.94 % | elapsed time = 455.09 (s) ]\n",
            "09/08/2020 04:27:43 AM: [ train: Epoch = 0 | iter = 1300/2012 | loss = 1.437 | batch P@1 = 54.07 % | elapsed time = 491.58 (s) ]\n",
            "09/08/2020 04:28:18 AM: [ train: Epoch = 0 | iter = 1400/2012 | loss = 1.475 | batch P@1 = 54.04 % | elapsed time = 526.39 (s) ]\n",
            "09/08/2020 04:28:53 AM: [ train: Epoch = 0 | iter = 1500/2012 | loss = 1.468 | batch P@1 = 54.01 % | elapsed time = 561.84 (s) ]\n",
            "09/08/2020 04:29:29 AM: [ train: Epoch = 0 | iter = 1600/2012 | loss = 1.430 | batch P@1 = 54.14 % | elapsed time = 598.10 (s) ]\n",
            "09/08/2020 04:30:05 AM: [ train: Epoch = 0 | iter = 1700/2012 | loss = 1.432 | batch P@1 = 54.17 % | elapsed time = 634.27 (s) ]\n",
            "09/08/2020 04:30:40 AM: [ train: Epoch = 0 | iter = 1800/2012 | loss = 1.413 | batch P@1 = 54.26 % | elapsed time = 669.17 (s) ]\n",
            "09/08/2020 04:31:16 AM: [ train: Epoch = 0 | iter = 1900/2012 | loss = 1.447 | batch P@1 = 54.35 % | elapsed time = 704.75 (s) ]\n",
            "09/08/2020 04:31:52 AM: [ train: Epoch = 0 | iter = 2000/2012 | loss = 1.426 | batch P@1 = 54.43 % | elapsed time = 740.43 (s) ]\n",
            "09/08/2020 04:31:56 AM: [ train: Epoch = 0 | iter = 2012/2012 | loss = 1.452 | batch P@1 = 54.44 % | elapsed time = 744.52 (s) ]\n",
            "09/08/2020 04:31:56 AM: [ train: Epoch 0 done. Time for epoch = 715.02 (s) ]\n",
            "09/08/2020 04:32:25 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 04:32:25 AM: [ Valid (shuffled): Epoch = 0 | avg loss = 1.405 | batch P@1 = 57.95 % | P@1,100 = 35.49% | P@3,100 = 54.91% | P@10,100 = 76.09% | valid time = 29.30 (s) ]\n",
            "09/08/2020 04:32:25 AM: [ New best loss, saving model to models/bert_auto.pt ]\n",
            "09/08/2020 04:33:05 AM: [ train: Epoch = 1 | iter = 100/2012 | loss = 1.316 | batch P@1 = 59.05 % | elapsed time = 813.69 (s) ]\n",
            "09/08/2020 04:33:40 AM: [ train: Epoch = 1 | iter = 200/2012 | loss = 1.335 | batch P@1 = 59.00 % | elapsed time = 848.77 (s) ]\n",
            "09/08/2020 04:34:16 AM: [ train: Epoch = 1 | iter = 300/2012 | loss = 1.324 | batch P@1 = 58.77 % | elapsed time = 885.22 (s) ]\n",
            "09/08/2020 04:34:52 AM: [ train: Epoch = 1 | iter = 400/2012 | loss = 1.294 | batch P@1 = 58.84 % | elapsed time = 921.21 (s) ]\n",
            "09/08/2020 04:35:28 AM: [ train: Epoch = 1 | iter = 500/2012 | loss = 1.339 | batch P@1 = 58.70 % | elapsed time = 957.13 (s) ]\n",
            "09/08/2020 04:36:04 AM: [ train: Epoch = 1 | iter = 600/2012 | loss = 1.333 | batch P@1 = 58.64 % | elapsed time = 992.97 (s) ]\n",
            "09/08/2020 04:36:41 AM: [ train: Epoch = 1 | iter = 700/2012 | loss = 1.308 | batch P@1 = 58.72 % | elapsed time = 1029.60 (s) ]\n",
            "09/08/2020 04:37:16 AM: [ train: Epoch = 1 | iter = 800/2012 | loss = 1.351 | batch P@1 = 58.78 % | elapsed time = 1065.11 (s) ]\n",
            "09/08/2020 04:37:51 AM: [ train: Epoch = 1 | iter = 900/2012 | loss = 1.373 | batch P@1 = 58.44 % | elapsed time = 1099.87 (s) ]\n",
            "09/08/2020 04:38:27 AM: [ train: Epoch = 1 | iter = 1000/2012 | loss = 1.358 | batch P@1 = 58.42 % | elapsed time = 1135.52 (s) ]\n",
            "09/08/2020 04:39:02 AM: [ train: Epoch = 1 | iter = 1100/2012 | loss = 1.353 | batch P@1 = 58.37 % | elapsed time = 1170.92 (s) ]\n",
            "09/08/2020 04:39:37 AM: [ train: Epoch = 1 | iter = 1200/2012 | loss = 1.329 | batch P@1 = 58.47 % | elapsed time = 1206.09 (s) ]\n",
            "09/08/2020 04:40:12 AM: [ train: Epoch = 1 | iter = 1300/2012 | loss = 1.347 | batch P@1 = 58.38 % | elapsed time = 1240.99 (s) ]\n",
            "09/08/2020 04:40:48 AM: [ train: Epoch = 1 | iter = 1400/2012 | loss = 1.319 | batch P@1 = 58.31 % | elapsed time = 1276.88 (s) ]\n",
            "09/08/2020 04:41:23 AM: [ train: Epoch = 1 | iter = 1500/2012 | loss = 1.314 | batch P@1 = 58.34 % | elapsed time = 1312.06 (s) ]\n",
            "09/08/2020 04:41:57 AM: [ train: Epoch = 1 | iter = 1600/2012 | loss = 1.353 | batch P@1 = 58.40 % | elapsed time = 1345.90 (s) ]\n",
            "09/08/2020 04:42:34 AM: [ train: Epoch = 1 | iter = 1700/2012 | loss = 1.327 | batch P@1 = 58.46 % | elapsed time = 1382.34 (s) ]\n",
            "09/08/2020 04:43:09 AM: [ train: Epoch = 1 | iter = 1800/2012 | loss = 1.287 | batch P@1 = 58.56 % | elapsed time = 1417.74 (s) ]\n",
            "09/08/2020 04:43:44 AM: [ train: Epoch = 1 | iter = 1900/2012 | loss = 1.276 | batch P@1 = 58.68 % | elapsed time = 1453.19 (s) ]\n",
            "09/08/2020 04:44:20 AM: [ train: Epoch = 1 | iter = 2000/2012 | loss = 1.290 | batch P@1 = 58.72 % | elapsed time = 1488.63 (s) ]\n",
            "09/08/2020 04:44:24 AM: [ train: Epoch = 1 | iter = 2012/2012 | loss = 1.457 | batch P@1 = 58.69 % | elapsed time = 1493.16 (s) ]\n",
            "09/08/2020 04:44:24 AM: [ train: Epoch 1 done. Time for epoch = 715.54 (s) ]\n",
            "09/08/2020 04:44:54 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 04:44:54 AM: [ Valid (shuffled): Epoch = 1 | avg loss = 1.326 | batch P@1 = 59.92 % | P@1,100 = 37.25% | P@3,100 = 57.21% | P@10,100 = 78.04% | valid time = 29.48 (s) ]\n",
            "09/08/2020 04:44:54 AM: [ New best loss, saving model to models/bert_auto.pt ]\n",
            "09/08/2020 04:45:33 AM: [ train: Epoch = 2 | iter = 100/2012 | loss = 1.188 | batch P@1 = 63.60 % | elapsed time = 1562.14 (s) ]\n",
            "09/08/2020 04:46:08 AM: [ train: Epoch = 2 | iter = 200/2012 | loss = 1.184 | batch P@1 = 63.38 % | elapsed time = 1597.17 (s) ]\n",
            "09/08/2020 04:46:44 AM: [ train: Epoch = 2 | iter = 300/2012 | loss = 1.195 | batch P@1 = 62.83 % | elapsed time = 1632.52 (s) ]\n",
            "09/08/2020 04:47:19 AM: [ train: Epoch = 2 | iter = 400/2012 | loss = 1.238 | batch P@1 = 62.19 % | elapsed time = 1667.64 (s) ]\n",
            "09/08/2020 04:47:55 AM: [ train: Epoch = 2 | iter = 500/2012 | loss = 1.235 | batch P@1 = 62.04 % | elapsed time = 1703.40 (s) ]\n",
            "09/08/2020 04:48:29 AM: [ train: Epoch = 2 | iter = 600/2012 | loss = 1.204 | batch P@1 = 62.01 % | elapsed time = 1737.96 (s) ]\n",
            "09/08/2020 04:49:04 AM: [ train: Epoch = 2 | iter = 700/2012 | loss = 1.231 | batch P@1 = 61.91 % | elapsed time = 1772.65 (s) ]\n",
            "09/08/2020 04:49:39 AM: [ train: Epoch = 2 | iter = 800/2012 | loss = 1.180 | batch P@1 = 62.01 % | elapsed time = 1808.27 (s) ]\n",
            "09/08/2020 04:50:16 AM: [ train: Epoch = 2 | iter = 900/2012 | loss = 1.210 | batch P@1 = 62.06 % | elapsed time = 1844.30 (s) ]\n",
            "09/08/2020 04:50:51 AM: [ train: Epoch = 2 | iter = 1000/2012 | loss = 1.207 | batch P@1 = 62.01 % | elapsed time = 1879.63 (s) ]\n",
            "09/08/2020 04:51:26 AM: [ train: Epoch = 2 | iter = 1100/2012 | loss = 1.222 | batch P@1 = 61.95 % | elapsed time = 1914.67 (s) ]\n",
            "09/08/2020 04:52:01 AM: [ train: Epoch = 2 | iter = 1200/2012 | loss = 1.237 | batch P@1 = 61.90 % | elapsed time = 1949.54 (s) ]\n",
            "09/08/2020 04:52:37 AM: [ train: Epoch = 2 | iter = 1300/2012 | loss = 1.269 | batch P@1 = 61.76 % | elapsed time = 1986.02 (s) ]\n",
            "09/08/2020 04:53:14 AM: [ train: Epoch = 2 | iter = 1400/2012 | loss = 1.234 | batch P@1 = 61.73 % | elapsed time = 2022.34 (s) ]\n",
            "09/08/2020 04:53:50 AM: [ train: Epoch = 2 | iter = 1500/2012 | loss = 1.231 | batch P@1 = 61.72 % | elapsed time = 2058.76 (s) ]\n",
            "09/08/2020 04:54:24 AM: [ train: Epoch = 2 | iter = 1600/2012 | loss = 1.189 | batch P@1 = 61.75 % | elapsed time = 2092.87 (s) ]\n",
            "09/08/2020 04:55:00 AM: [ train: Epoch = 2 | iter = 1700/2012 | loss = 1.162 | batch P@1 = 61.83 % | elapsed time = 2128.50 (s) ]\n",
            "09/08/2020 04:55:35 AM: [ train: Epoch = 2 | iter = 1800/2012 | loss = 1.227 | batch P@1 = 61.80 % | elapsed time = 2163.97 (s) ]\n",
            "09/08/2020 04:56:11 AM: [ train: Epoch = 2 | iter = 1900/2012 | loss = 1.184 | batch P@1 = 61.87 % | elapsed time = 2199.36 (s) ]\n",
            "09/08/2020 04:56:47 AM: [ train: Epoch = 2 | iter = 2000/2012 | loss = 1.121 | batch P@1 = 62.00 % | elapsed time = 2235.73 (s) ]\n",
            "09/08/2020 04:56:51 AM: [ train: Epoch = 2 | iter = 2012/2012 | loss = 1.137 | batch P@1 = 62.03 % | elapsed time = 2239.90 (s) ]\n",
            "09/08/2020 04:56:51 AM: [ train: Epoch 2 done. Time for epoch = 713.76 (s) ]\n",
            "09/08/2020 04:57:20 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 04:57:20 AM: [ Valid (shuffled): Epoch = 2 | avg loss = 1.278 | batch P@1 = 61.75 % | P@1,100 = 39.46% | P@3,100 = 59.02% | P@10,100 = 79.79% | valid time = 29.32 (s) ]\n",
            "09/08/2020 04:57:20 AM: [ New best loss, saving model to models/bert_auto.pt ]\n",
            "09/08/2020 04:58:00 AM: [ train: Epoch = 3 | iter = 100/2012 | loss = 1.101 | batch P@1 = 64.95 % | elapsed time = 2308.67 (s) ]\n",
            "09/08/2020 04:58:36 AM: [ train: Epoch = 3 | iter = 200/2012 | loss = 1.116 | batch P@1 = 64.15 % | elapsed time = 2345.03 (s) ]\n",
            "09/08/2020 04:59:12 AM: [ train: Epoch = 3 | iter = 300/2012 | loss = 1.124 | batch P@1 = 64.23 % | elapsed time = 2380.98 (s) ]\n",
            "09/08/2020 04:59:48 AM: [ train: Epoch = 3 | iter = 400/2012 | loss = 1.141 | batch P@1 = 63.98 % | elapsed time = 2416.68 (s) ]\n",
            "09/08/2020 05:00:24 AM: [ train: Epoch = 3 | iter = 500/2012 | loss = 1.098 | batch P@1 = 64.40 % | elapsed time = 2452.39 (s) ]\n",
            "09/08/2020 05:00:58 AM: [ train: Epoch = 3 | iter = 600/2012 | loss = 1.106 | batch P@1 = 64.38 % | elapsed time = 2487.23 (s) ]\n",
            "09/08/2020 05:01:33 AM: [ train: Epoch = 3 | iter = 700/2012 | loss = 1.139 | batch P@1 = 64.27 % | elapsed time = 2521.88 (s) ]\n",
            "09/08/2020 05:02:09 AM: [ train: Epoch = 3 | iter = 800/2012 | loss = 1.124 | batch P@1 = 64.21 % | elapsed time = 2557.87 (s) ]\n",
            "09/08/2020 05:02:44 AM: [ train: Epoch = 3 | iter = 900/2012 | loss = 1.084 | batch P@1 = 64.40 % | elapsed time = 2592.56 (s) ]\n",
            "09/08/2020 05:03:18 AM: [ train: Epoch = 3 | iter = 1000/2012 | loss = 1.097 | batch P@1 = 64.35 % | elapsed time = 2627.14 (s) ]\n",
            "09/08/2020 05:03:54 AM: [ train: Epoch = 3 | iter = 1100/2012 | loss = 1.106 | batch P@1 = 64.46 % | elapsed time = 2663.26 (s) ]\n",
            "09/08/2020 05:04:29 AM: [ train: Epoch = 3 | iter = 1200/2012 | loss = 1.093 | batch P@1 = 64.58 % | elapsed time = 2698.18 (s) ]\n",
            "09/08/2020 05:05:05 AM: [ train: Epoch = 3 | iter = 1300/2012 | loss = 1.119 | batch P@1 = 64.55 % | elapsed time = 2734.27 (s) ]\n",
            "09/08/2020 05:05:41 AM: [ train: Epoch = 3 | iter = 1400/2012 | loss = 1.086 | batch P@1 = 64.63 % | elapsed time = 2770.16 (s) ]\n",
            "09/08/2020 05:06:16 AM: [ train: Epoch = 3 | iter = 1500/2012 | loss = 1.095 | batch P@1 = 64.64 % | elapsed time = 2805.11 (s) ]\n",
            "09/08/2020 05:06:52 AM: [ train: Epoch = 3 | iter = 1600/2012 | loss = 1.089 | batch P@1 = 64.69 % | elapsed time = 2840.69 (s) ]\n",
            "09/08/2020 05:07:27 AM: [ train: Epoch = 3 | iter = 1700/2012 | loss = 1.097 | batch P@1 = 64.79 % | elapsed time = 2876.03 (s) ]\n",
            "09/08/2020 05:08:04 AM: [ train: Epoch = 3 | iter = 1800/2012 | loss = 1.107 | batch P@1 = 64.72 % | elapsed time = 2912.86 (s) ]\n",
            "09/08/2020 05:08:41 AM: [ train: Epoch = 3 | iter = 1900/2012 | loss = 1.070 | batch P@1 = 64.76 % | elapsed time = 2949.31 (s) ]\n",
            "09/08/2020 05:09:16 AM: [ train: Epoch = 3 | iter = 2000/2012 | loss = 1.082 | batch P@1 = 64.81 % | elapsed time = 2984.32 (s) ]\n",
            "09/08/2020 05:09:20 AM: [ train: Epoch = 3 | iter = 2012/2012 | loss = 1.311 | batch P@1 = 64.78 % | elapsed time = 2988.58 (s) ]\n",
            "09/08/2020 05:09:20 AM: [ train: Epoch 3 done. Time for epoch = 715.89 (s) ]\n",
            "09/08/2020 05:09:49 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 05:09:49 AM: [ Valid (shuffled): Epoch = 3 | avg loss = 1.229 | batch P@1 = 62.64 % | P@1,100 = 39.84% | P@3,100 = 59.81% | P@10,100 = 81.14% | valid time = 29.03 (s) ]\n",
            "09/08/2020 05:09:49 AM: [ New best loss, saving model to models/bert_auto.pt ]\n",
            "09/08/2020 05:10:29 AM: [ train: Epoch = 4 | iter = 100/2012 | loss = 0.957 | batch P@1 = 68.25 % | elapsed time = 3057.76 (s) ]\n",
            "09/08/2020 05:11:05 AM: [ train: Epoch = 4 | iter = 200/2012 | loss = 1.003 | batch P@1 = 68.20 % | elapsed time = 3093.56 (s) ]\n",
            "09/08/2020 05:11:40 AM: [ train: Epoch = 4 | iter = 300/2012 | loss = 1.018 | batch P@1 = 68.18 % | elapsed time = 3128.65 (s) ]\n",
            "09/08/2020 05:12:16 AM: [ train: Epoch = 4 | iter = 400/2012 | loss = 1.000 | batch P@1 = 67.84 % | elapsed time = 3165.20 (s) ]\n",
            "09/08/2020 05:12:53 AM: [ train: Epoch = 4 | iter = 500/2012 | loss = 1.032 | batch P@1 = 67.64 % | elapsed time = 3201.33 (s) ]\n",
            "09/08/2020 05:13:27 AM: [ train: Epoch = 4 | iter = 600/2012 | loss = 1.054 | batch P@1 = 67.35 % | elapsed time = 3235.41 (s) ]\n",
            "09/08/2020 05:14:03 AM: [ train: Epoch = 4 | iter = 700/2012 | loss = 1.002 | batch P@1 = 67.41 % | elapsed time = 3271.34 (s) ]\n",
            "09/08/2020 05:14:38 AM: [ train: Epoch = 4 | iter = 800/2012 | loss = 1.057 | batch P@1 = 67.13 % | elapsed time = 3307.02 (s) ]\n",
            "09/08/2020 05:15:13 AM: [ train: Epoch = 4 | iter = 900/2012 | loss = 1.041 | batch P@1 = 67.16 % | elapsed time = 3341.87 (s) ]\n",
            "09/08/2020 05:15:48 AM: [ train: Epoch = 4 | iter = 1000/2012 | loss = 1.073 | batch P@1 = 67.03 % | elapsed time = 3377.15 (s) ]\n",
            "09/08/2020 05:16:23 AM: [ train: Epoch = 4 | iter = 1100/2012 | loss = 1.083 | batch P@1 = 66.90 % | elapsed time = 3411.88 (s) ]\n",
            "09/08/2020 05:16:59 AM: [ train: Epoch = 4 | iter = 1200/2012 | loss = 1.086 | batch P@1 = 66.76 % | elapsed time = 3447.65 (s) ]\n",
            "09/08/2020 05:17:36 AM: [ train: Epoch = 4 | iter = 1300/2012 | loss = 1.038 | batch P@1 = 66.70 % | elapsed time = 3484.53 (s) ]\n",
            "09/08/2020 05:18:11 AM: [ train: Epoch = 4 | iter = 1400/2012 | loss = 1.078 | batch P@1 = 66.68 % | elapsed time = 3519.84 (s) ]\n",
            "09/08/2020 05:18:46 AM: [ train: Epoch = 4 | iter = 1500/2012 | loss = 1.050 | batch P@1 = 66.64 % | elapsed time = 3555.24 (s) ]\n",
            "09/08/2020 05:19:21 AM: [ train: Epoch = 4 | iter = 1600/2012 | loss = 1.018 | batch P@1 = 66.71 % | elapsed time = 3589.72 (s) ]\n",
            "09/08/2020 05:19:56 AM: [ train: Epoch = 4 | iter = 1700/2012 | loss = 1.019 | batch P@1 = 66.75 % | elapsed time = 3624.77 (s) ]\n",
            "09/08/2020 05:20:32 AM: [ train: Epoch = 4 | iter = 1800/2012 | loss = 1.010 | batch P@1 = 66.80 % | elapsed time = 3661.13 (s) ]\n",
            "09/08/2020 05:21:08 AM: [ train: Epoch = 4 | iter = 1900/2012 | loss = 1.006 | batch P@1 = 66.81 % | elapsed time = 3696.38 (s) ]\n",
            "09/08/2020 05:21:44 AM: [ train: Epoch = 4 | iter = 2000/2012 | loss = 1.006 | batch P@1 = 66.77 % | elapsed time = 3732.30 (s) ]\n",
            "09/08/2020 05:21:48 AM: [ train: Epoch = 4 | iter = 2012/2012 | loss = 0.910 | batch P@1 = 66.79 % | elapsed time = 3736.83 (s) ]\n",
            "09/08/2020 05:21:48 AM: [ train: Epoch 4 done. Time for epoch = 715.73 (s) ]\n",
            "09/08/2020 05:22:17 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 05:22:17 AM: [ Valid (shuffled): Epoch = 4 | avg loss = 1.199 | batch P@1 = 62.90 % | P@1,100 = 40.86% | P@3,100 = 60.65% | P@10,100 = 81.58% | valid time = 29.21 (s) ]\n",
            "09/08/2020 05:22:17 AM: [ New best loss, saving model to models/bert_auto.pt ]\n",
            "09/08/2020 05:22:57 AM: [ train: Epoch = 5 | iter = 100/2012 | loss = 0.944 | batch P@1 = 69.65 % | elapsed time = 3805.79 (s) ]\n",
            "09/08/2020 05:23:33 AM: [ train: Epoch = 5 | iter = 200/2012 | loss = 0.969 | batch P@1 = 69.58 % | elapsed time = 3841.87 (s) ]\n",
            "09/08/2020 05:24:09 AM: [ train: Epoch = 5 | iter = 300/2012 | loss = 0.943 | batch P@1 = 69.58 % | elapsed time = 3877.55 (s) ]\n",
            "09/08/2020 05:24:45 AM: [ train: Epoch = 5 | iter = 400/2012 | loss = 0.933 | batch P@1 = 69.48 % | elapsed time = 3913.87 (s) ]\n",
            "09/08/2020 05:25:21 AM: [ train: Epoch = 5 | iter = 500/2012 | loss = 0.926 | batch P@1 = 69.64 % | elapsed time = 3949.57 (s) ]\n",
            "09/08/2020 05:25:57 AM: [ train: Epoch = 5 | iter = 600/2012 | loss = 0.961 | batch P@1 = 69.49 % | elapsed time = 3985.85 (s) ]\n",
            "09/08/2020 05:26:33 AM: [ train: Epoch = 5 | iter = 700/2012 | loss = 0.981 | batch P@1 = 69.35 % | elapsed time = 4021.61 (s) ]\n",
            "09/08/2020 05:27:08 AM: [ train: Epoch = 5 | iter = 800/2012 | loss = 0.968 | batch P@1 = 69.29 % | elapsed time = 4056.38 (s) ]\n",
            "09/08/2020 05:27:43 AM: [ train: Epoch = 5 | iter = 900/2012 | loss = 0.927 | batch P@1 = 69.34 % | elapsed time = 4091.55 (s) ]\n",
            "09/08/2020 05:28:19 AM: [ train: Epoch = 5 | iter = 1000/2012 | loss = 0.962 | batch P@1 = 69.21 % | elapsed time = 4127.73 (s) ]\n",
            "09/08/2020 05:28:54 AM: [ train: Epoch = 5 | iter = 1100/2012 | loss = 1.048 | batch P@1 = 68.85 % | elapsed time = 4163.12 (s) ]\n",
            "09/08/2020 05:29:29 AM: [ train: Epoch = 5 | iter = 1200/2012 | loss = 0.932 | batch P@1 = 68.90 % | elapsed time = 4197.73 (s) ]\n",
            "09/08/2020 05:30:06 AM: [ train: Epoch = 5 | iter = 1300/2012 | loss = 0.947 | batch P@1 = 68.99 % | elapsed time = 4234.36 (s) ]\n",
            "09/08/2020 05:30:40 AM: [ train: Epoch = 5 | iter = 1400/2012 | loss = 0.947 | batch P@1 = 69.02 % | elapsed time = 4268.74 (s) ]\n",
            "09/08/2020 05:31:15 AM: [ train: Epoch = 5 | iter = 1500/2012 | loss = 0.970 | batch P@1 = 68.95 % | elapsed time = 4303.75 (s) ]\n",
            "09/08/2020 05:31:50 AM: [ train: Epoch = 5 | iter = 1600/2012 | loss = 0.976 | batch P@1 = 68.93 % | elapsed time = 4338.80 (s) ]\n",
            "09/08/2020 05:32:26 AM: [ train: Epoch = 5 | iter = 1700/2012 | loss = 0.953 | batch P@1 = 68.97 % | elapsed time = 4374.82 (s) ]\n",
            "09/08/2020 05:33:02 AM: [ train: Epoch = 5 | iter = 1800/2012 | loss = 0.978 | batch P@1 = 68.97 % | elapsed time = 4410.72 (s) ]\n",
            "09/08/2020 05:33:37 AM: [ train: Epoch = 5 | iter = 1900/2012 | loss = 0.944 | batch P@1 = 68.99 % | elapsed time = 4445.99 (s) ]\n",
            "09/08/2020 05:34:13 AM: [ train: Epoch = 5 | iter = 2000/2012 | loss = 0.924 | batch P@1 = 69.04 % | elapsed time = 4481.62 (s) ]\n",
            "09/08/2020 05:34:17 AM: [ train: Epoch = 5 | iter = 2012/2012 | loss = 1.027 | batch P@1 = 69.03 % | elapsed time = 4485.50 (s) ]\n",
            "09/08/2020 05:34:17 AM: [ train: Epoch 5 done. Time for epoch = 716.01 (s) ]\n",
            "09/08/2020 05:34:46 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 05:34:46 AM: [ Valid (shuffled): Epoch = 5 | avg loss = 1.191 | batch P@1 = 64.12 % | P@1,100 = 41.07% | P@3,100 = 60.96% | P@10,100 = 82.25% | valid time = 29.39 (s) ]\n",
            "09/08/2020 05:34:46 AM: [ New best loss, saving model to models/bert_auto.pt ]\n",
            "09/08/2020 05:35:26 AM: [ train: Epoch = 6 | iter = 100/2012 | loss = 0.894 | batch P@1 = 70.75 % | elapsed time = 4554.45 (s) ]\n",
            "09/08/2020 05:36:02 AM: [ train: Epoch = 6 | iter = 200/2012 | loss = 0.874 | batch P@1 = 71.05 % | elapsed time = 4590.61 (s) ]\n",
            "09/08/2020 05:36:38 AM: [ train: Epoch = 6 | iter = 300/2012 | loss = 0.940 | batch P@1 = 70.58 % | elapsed time = 4626.58 (s) ]\n",
            "09/08/2020 05:37:12 AM: [ train: Epoch = 6 | iter = 400/2012 | loss = 0.873 | batch P@1 = 70.68 % | elapsed time = 4660.79 (s) ]\n",
            "09/08/2020 05:37:48 AM: [ train: Epoch = 6 | iter = 500/2012 | loss = 0.869 | batch P@1 = 70.82 % | elapsed time = 4696.78 (s) ]\n",
            "09/08/2020 05:38:24 AM: [ train: Epoch = 6 | iter = 600/2012 | loss = 0.902 | batch P@1 = 70.75 % | elapsed time = 4732.33 (s) ]\n",
            "09/08/2020 05:38:59 AM: [ train: Epoch = 6 | iter = 700/2012 | loss = 0.879 | batch P@1 = 70.83 % | elapsed time = 4767.72 (s) ]\n",
            "09/08/2020 05:39:35 AM: [ train: Epoch = 6 | iter = 800/2012 | loss = 0.927 | batch P@1 = 70.66 % | elapsed time = 4804.17 (s) ]\n",
            "09/08/2020 05:40:11 AM: [ train: Epoch = 6 | iter = 900/2012 | loss = 0.934 | batch P@1 = 70.39 % | elapsed time = 4839.87 (s) ]\n",
            "09/08/2020 05:40:46 AM: [ train: Epoch = 6 | iter = 1000/2012 | loss = 0.868 | batch P@1 = 70.57 % | elapsed time = 4875.14 (s) ]\n",
            "09/08/2020 05:41:23 AM: [ train: Epoch = 6 | iter = 1100/2012 | loss = 0.876 | batch P@1 = 70.65 % | elapsed time = 4911.66 (s) ]\n",
            "09/08/2020 05:41:58 AM: [ train: Epoch = 6 | iter = 1200/2012 | loss = 0.915 | batch P@1 = 70.55 % | elapsed time = 4946.91 (s) ]\n",
            "09/08/2020 05:42:34 AM: [ train: Epoch = 6 | iter = 1300/2012 | loss = 0.914 | batch P@1 = 70.60 % | elapsed time = 4982.67 (s) ]\n",
            "09/08/2020 05:43:09 AM: [ train: Epoch = 6 | iter = 1400/2012 | loss = 0.938 | batch P@1 = 70.53 % | elapsed time = 5018.14 (s) ]\n",
            "09/08/2020 05:43:43 AM: [ train: Epoch = 6 | iter = 1500/2012 | loss = 0.913 | batch P@1 = 70.56 % | elapsed time = 5052.28 (s) ]\n",
            "09/08/2020 05:44:18 AM: [ train: Epoch = 6 | iter = 1600/2012 | loss = 0.902 | batch P@1 = 70.62 % | elapsed time = 5086.42 (s) ]\n",
            "09/08/2020 05:44:52 AM: [ train: Epoch = 6 | iter = 1700/2012 | loss = 0.876 | batch P@1 = 70.69 % | elapsed time = 5121.15 (s) ]\n",
            "09/08/2020 05:45:28 AM: [ train: Epoch = 6 | iter = 1800/2012 | loss = 0.870 | batch P@1 = 70.76 % | elapsed time = 5156.73 (s) ]\n",
            "09/08/2020 05:46:04 AM: [ train: Epoch = 6 | iter = 1900/2012 | loss = 0.915 | batch P@1 = 70.71 % | elapsed time = 5193.19 (s) ]\n",
            "09/08/2020 05:46:41 AM: [ train: Epoch = 6 | iter = 2000/2012 | loss = 0.877 | batch P@1 = 70.70 % | elapsed time = 5229.43 (s) ]\n",
            "09/08/2020 05:46:45 AM: [ train: Epoch = 6 | iter = 2012/2012 | loss = 0.942 | batch P@1 = 70.68 % | elapsed time = 5233.83 (s) ]\n",
            "09/08/2020 05:46:45 AM: [ train: Epoch 6 done. Time for epoch = 715.24 (s) ]\n",
            "09/08/2020 05:47:14 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 05:47:15 AM: [ Valid (shuffled): Epoch = 6 | avg loss = 1.174 | batch P@1 = 64.62 % | P@1,100 = 42.28% | P@3,100 = 62.23% | P@10,100 = 82.46% | valid time = 29.45 (s) ]\n",
            "09/08/2020 05:47:15 AM: [ New best loss, saving model to models/bert_auto.pt ]\n",
            "09/08/2020 05:47:53 AM: [ train: Epoch = 7 | iter = 100/2012 | loss = 0.801 | batch P@1 = 73.55 % | elapsed time = 5302.24 (s) ]\n",
            "09/08/2020 05:48:29 AM: [ train: Epoch = 7 | iter = 200/2012 | loss = 0.790 | batch P@1 = 73.45 % | elapsed time = 5337.89 (s) ]\n",
            "09/08/2020 05:49:04 AM: [ train: Epoch = 7 | iter = 300/2012 | loss = 0.880 | batch P@1 = 73.08 % | elapsed time = 5372.49 (s) ]\n",
            "09/08/2020 05:49:39 AM: [ train: Epoch = 7 | iter = 400/2012 | loss = 0.806 | batch P@1 = 72.93 % | elapsed time = 5407.68 (s) ]\n",
            "09/08/2020 05:50:15 AM: [ train: Epoch = 7 | iter = 500/2012 | loss = 0.811 | batch P@1 = 73.11 % | elapsed time = 5443.99 (s) ]\n",
            "09/08/2020 05:50:51 AM: [ train: Epoch = 7 | iter = 600/2012 | loss = 0.801 | batch P@1 = 73.22 % | elapsed time = 5479.69 (s) ]\n",
            "09/08/2020 05:51:28 AM: [ train: Epoch = 7 | iter = 700/2012 | loss = 0.789 | batch P@1 = 73.35 % | elapsed time = 5517.04 (s) ]\n",
            "09/08/2020 05:52:03 AM: [ train: Epoch = 7 | iter = 800/2012 | loss = 0.869 | batch P@1 = 73.12 % | elapsed time = 5551.98 (s) ]\n",
            "09/08/2020 05:52:38 AM: [ train: Epoch = 7 | iter = 900/2012 | loss = 0.814 | batch P@1 = 73.13 % | elapsed time = 5586.41 (s) ]\n",
            "09/08/2020 05:53:13 AM: [ train: Epoch = 7 | iter = 1000/2012 | loss = 0.849 | batch P@1 = 73.00 % | elapsed time = 5621.92 (s) ]\n",
            "09/08/2020 05:53:49 AM: [ train: Epoch = 7 | iter = 1100/2012 | loss = 0.826 | batch P@1 = 72.95 % | elapsed time = 5657.85 (s) ]\n",
            "09/08/2020 05:54:23 AM: [ train: Epoch = 7 | iter = 1200/2012 | loss = 0.850 | batch P@1 = 72.90 % | elapsed time = 5692.28 (s) ]\n",
            "09/08/2020 05:54:59 AM: [ train: Epoch = 7 | iter = 1300/2012 | loss = 0.843 | batch P@1 = 72.92 % | elapsed time = 5728.28 (s) ]\n",
            "09/08/2020 05:55:35 AM: [ train: Epoch = 7 | iter = 1400/2012 | loss = 0.856 | batch P@1 = 72.82 % | elapsed time = 5763.70 (s) ]\n",
            "09/08/2020 05:56:11 AM: [ train: Epoch = 7 | iter = 1500/2012 | loss = 0.833 | batch P@1 = 72.79 % | elapsed time = 5799.85 (s) ]\n",
            "09/08/2020 05:56:47 AM: [ train: Epoch = 7 | iter = 1600/2012 | loss = 0.802 | batch P@1 = 72.87 % | elapsed time = 5835.43 (s) ]\n",
            "09/08/2020 05:57:22 AM: [ train: Epoch = 7 | iter = 1700/2012 | loss = 0.826 | batch P@1 = 72.90 % | elapsed time = 5870.54 (s) ]\n",
            "09/08/2020 05:57:57 AM: [ train: Epoch = 7 | iter = 1800/2012 | loss = 0.849 | batch P@1 = 72.92 % | elapsed time = 5905.85 (s) ]\n",
            "09/08/2020 05:58:32 AM: [ train: Epoch = 7 | iter = 1900/2012 | loss = 0.818 | batch P@1 = 72.95 % | elapsed time = 5940.79 (s) ]\n",
            "09/08/2020 05:59:09 AM: [ train: Epoch = 7 | iter = 2000/2012 | loss = 0.808 | batch P@1 = 72.95 % | elapsed time = 5977.39 (s) ]\n",
            "09/08/2020 05:59:13 AM: [ train: Epoch = 7 | iter = 2012/2012 | loss = 0.794 | batch P@1 = 72.94 % | elapsed time = 5981.65 (s) ]\n",
            "09/08/2020 05:59:13 AM: [ train: Epoch 7 done. Time for epoch = 714.89 (s) ]\n",
            "09/08/2020 05:59:42 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 05:59:42 AM: [ Valid (shuffled): Epoch = 7 | avg loss = 1.172 | batch P@1 = 65.49 % | P@1,100 = 42.96% | P@3,100 = 63.28% | P@10,100 = 83.09% | valid time = 29.44 (s) ]\n",
            "09/08/2020 05:59:42 AM: [ New best loss, saving model to models/bert_auto.pt ]\n",
            "09/08/2020 06:00:22 AM: [ train: Epoch = 8 | iter = 100/2012 | loss = 0.733 | batch P@1 = 75.30 % | elapsed time = 6051.15 (s) ]\n",
            "09/08/2020 06:00:59 AM: [ train: Epoch = 8 | iter = 200/2012 | loss = 0.744 | batch P@1 = 75.05 % | elapsed time = 6087.80 (s) ]\n",
            "09/08/2020 06:01:33 AM: [ train: Epoch = 8 | iter = 300/2012 | loss = 0.754 | batch P@1 = 74.83 % | elapsed time = 6121.91 (s) ]\n",
            "09/08/2020 06:02:08 AM: [ train: Epoch = 8 | iter = 400/2012 | loss = 0.734 | batch P@1 = 74.98 % | elapsed time = 6157.01 (s) ]\n",
            "09/08/2020 06:02:43 AM: [ train: Epoch = 8 | iter = 500/2012 | loss = 0.778 | batch P@1 = 74.86 % | elapsed time = 6191.83 (s) ]\n",
            "09/08/2020 06:03:19 AM: [ train: Epoch = 8 | iter = 600/2012 | loss = 0.749 | batch P@1 = 74.89 % | elapsed time = 6227.48 (s) ]\n",
            "09/08/2020 06:03:53 AM: [ train: Epoch = 8 | iter = 700/2012 | loss = 0.791 | batch P@1 = 74.91 % | elapsed time = 6262.07 (s) ]\n",
            "09/08/2020 06:04:30 AM: [ train: Epoch = 8 | iter = 800/2012 | loss = 0.763 | batch P@1 = 74.99 % | elapsed time = 6298.41 (s) ]\n",
            "09/08/2020 06:05:05 AM: [ train: Epoch = 8 | iter = 900/2012 | loss = 0.797 | batch P@1 = 74.89 % | elapsed time = 6334.21 (s) ]\n",
            "09/08/2020 06:05:41 AM: [ train: Epoch = 8 | iter = 1000/2012 | loss = 0.795 | batch P@1 = 74.67 % | elapsed time = 6369.66 (s) ]\n",
            "09/08/2020 06:06:15 AM: [ train: Epoch = 8 | iter = 1100/2012 | loss = 0.834 | batch P@1 = 74.41 % | elapsed time = 6403.87 (s) ]\n",
            "09/08/2020 06:06:50 AM: [ train: Epoch = 8 | iter = 1200/2012 | loss = 0.797 | batch P@1 = 74.42 % | elapsed time = 6438.88 (s) ]\n",
            "09/08/2020 06:07:26 AM: [ train: Epoch = 8 | iter = 1300/2012 | loss = 0.738 | batch P@1 = 74.52 % | elapsed time = 6475.12 (s) ]\n",
            "09/08/2020 06:08:02 AM: [ train: Epoch = 8 | iter = 1400/2012 | loss = 0.821 | batch P@1 = 74.38 % | elapsed time = 6510.49 (s) ]\n",
            "09/08/2020 06:08:38 AM: [ train: Epoch = 8 | iter = 1500/2012 | loss = 0.827 | batch P@1 = 74.21 % | elapsed time = 6546.91 (s) ]\n",
            "09/08/2020 06:09:13 AM: [ train: Epoch = 8 | iter = 1600/2012 | loss = 0.758 | batch P@1 = 74.25 % | elapsed time = 6582.23 (s) ]\n",
            "09/08/2020 06:09:49 AM: [ train: Epoch = 8 | iter = 1700/2012 | loss = 0.766 | batch P@1 = 74.24 % | elapsed time = 6617.46 (s) ]\n",
            "09/08/2020 06:10:24 AM: [ train: Epoch = 8 | iter = 1800/2012 | loss = 0.779 | batch P@1 = 74.23 % | elapsed time = 6652.86 (s) ]\n",
            "09/08/2020 06:11:01 AM: [ train: Epoch = 8 | iter = 1900/2012 | loss = 0.824 | batch P@1 = 74.12 % | elapsed time = 6689.66 (s) ]\n",
            "09/08/2020 06:11:36 AM: [ train: Epoch = 8 | iter = 2000/2012 | loss = 0.763 | batch P@1 = 74.17 % | elapsed time = 6725.28 (s) ]\n",
            "09/08/2020 06:11:41 AM: [ train: Epoch = 8 | iter = 2012/2012 | loss = 0.916 | batch P@1 = 74.15 % | elapsed time = 6729.64 (s) ]\n",
            "09/08/2020 06:11:41 AM: [ train: Epoch 8 done. Time for epoch = 715.08 (s) ]\n",
            "09/08/2020 06:12:10 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 06:12:10 AM: [ Valid (shuffled): Epoch = 8 | avg loss = 1.181 | batch P@1 = 65.56 % | P@1,100 = 43.07% | P@3,100 = 63.07% | P@10,100 = 82.14% | valid time = 29.38 (s) ]\n",
            "09/08/2020 06:12:45 AM: [ train: Epoch = 9 | iter = 100/2012 | loss = 0.705 | batch P@1 = 76.55 % | elapsed time = 6793.65 (s) ]\n",
            "09/08/2020 06:13:21 AM: [ train: Epoch = 9 | iter = 200/2012 | loss = 0.708 | batch P@1 = 76.90 % | elapsed time = 6829.80 (s) ]\n",
            "09/08/2020 06:13:55 AM: [ train: Epoch = 9 | iter = 300/2012 | loss = 0.746 | batch P@1 = 76.42 % | elapsed time = 6863.91 (s) ]\n",
            "09/08/2020 06:14:30 AM: [ train: Epoch = 9 | iter = 400/2012 | loss = 0.736 | batch P@1 = 75.93 % | elapsed time = 6898.59 (s) ]\n",
            "09/08/2020 06:15:04 AM: [ train: Epoch = 9 | iter = 500/2012 | loss = 0.710 | batch P@1 = 75.86 % | elapsed time = 6932.70 (s) ]\n",
            "09/08/2020 06:15:39 AM: [ train: Epoch = 9 | iter = 600/2012 | loss = 0.789 | batch P@1 = 75.53 % | elapsed time = 6967.95 (s) ]\n",
            "09/08/2020 06:16:15 AM: [ train: Epoch = 9 | iter = 700/2012 | loss = 0.720 | batch P@1 = 75.55 % | elapsed time = 7003.93 (s) ]\n",
            "09/08/2020 06:16:51 AM: [ train: Epoch = 9 | iter = 800/2012 | loss = 0.736 | batch P@1 = 75.56 % | elapsed time = 7039.78 (s) ]\n",
            "09/08/2020 06:17:26 AM: [ train: Epoch = 9 | iter = 900/2012 | loss = 0.774 | batch P@1 = 75.32 % | elapsed time = 7074.62 (s) ]\n",
            "09/08/2020 06:18:02 AM: [ train: Epoch = 9 | iter = 1000/2012 | loss = 0.691 | batch P@1 = 75.47 % | elapsed time = 7110.89 (s) ]\n",
            "09/08/2020 06:18:38 AM: [ train: Epoch = 9 | iter = 1100/2012 | loss = 0.692 | batch P@1 = 75.60 % | elapsed time = 7146.50 (s) ]\n",
            "09/08/2020 06:19:15 AM: [ train: Epoch = 9 | iter = 1200/2012 | loss = 0.732 | batch P@1 = 75.66 % | elapsed time = 7183.57 (s) ]\n",
            "09/08/2020 06:19:49 AM: [ train: Epoch = 9 | iter = 1300/2012 | loss = 0.722 | batch P@1 = 75.68 % | elapsed time = 7218.25 (s) ]\n",
            "09/08/2020 06:20:24 AM: [ train: Epoch = 9 | iter = 1400/2012 | loss = 0.712 | batch P@1 = 75.72 % | elapsed time = 7252.38 (s) ]\n",
            "09/08/2020 06:20:59 AM: [ train: Epoch = 9 | iter = 1500/2012 | loss = 0.662 | batch P@1 = 75.92 % | elapsed time = 7287.47 (s) ]\n",
            "09/08/2020 06:21:34 AM: [ train: Epoch = 9 | iter = 1600/2012 | loss = 0.714 | batch P@1 = 75.89 % | elapsed time = 7323.02 (s) ]\n",
            "09/08/2020 06:22:10 AM: [ train: Epoch = 9 | iter = 1700/2012 | loss = 0.740 | batch P@1 = 75.89 % | elapsed time = 7358.81 (s) ]\n",
            "09/08/2020 06:22:47 AM: [ train: Epoch = 9 | iter = 1800/2012 | loss = 0.734 | batch P@1 = 75.86 % | elapsed time = 7395.31 (s) ]\n",
            "09/08/2020 06:23:23 AM: [ train: Epoch = 9 | iter = 1900/2012 | loss = 0.736 | batch P@1 = 75.81 % | elapsed time = 7431.74 (s) ]\n",
            "09/08/2020 06:23:59 AM: [ train: Epoch = 9 | iter = 2000/2012 | loss = 0.737 | batch P@1 = 75.79 % | elapsed time = 7467.32 (s) ]\n",
            "09/08/2020 06:24:03 AM: [ train: Epoch = 9 | iter = 2012/2012 | loss = 0.759 | batch P@1 = 75.78 % | elapsed time = 7471.69 (s) ]\n",
            "09/08/2020 06:24:03 AM: [ train: Epoch 9 done. Time for epoch = 712.65 (s) ]\n",
            "09/08/2020 06:24:32 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 06:24:32 AM: [ Valid (shuffled): Epoch = 9 | avg loss = 1.204 | batch P@1 = 65.02 % | P@1,100 = 42.18% | P@3,100 = 63.53% | P@10,100 = 83.42% | valid time = 29.27 (s) ]\n",
            "09/08/2020 06:25:09 AM: [ train: Epoch = 10 | iter = 100/2012 | loss = 0.666 | batch P@1 = 76.75 % | elapsed time = 7537.97 (s) ]\n",
            "09/08/2020 06:25:45 AM: [ train: Epoch = 10 | iter = 200/2012 | loss = 0.656 | batch P@1 = 76.93 % | elapsed time = 7573.48 (s) ]\n",
            "09/08/2020 06:26:20 AM: [ train: Epoch = 10 | iter = 300/2012 | loss = 0.732 | batch P@1 = 76.63 % | elapsed time = 7608.77 (s) ]\n",
            "09/08/2020 06:26:54 AM: [ train: Epoch = 10 | iter = 400/2012 | loss = 0.692 | batch P@1 = 76.73 % | elapsed time = 7642.93 (s) ]\n",
            "09/08/2020 06:27:30 AM: [ train: Epoch = 10 | iter = 500/2012 | loss = 0.644 | batch P@1 = 77.08 % | elapsed time = 7679.05 (s) ]\n",
            "09/08/2020 06:28:07 AM: [ train: Epoch = 10 | iter = 600/2012 | loss = 0.667 | batch P@1 = 77.21 % | elapsed time = 7715.29 (s) ]\n",
            "09/08/2020 06:28:41 AM: [ train: Epoch = 10 | iter = 700/2012 | loss = 0.689 | batch P@1 = 77.21 % | elapsed time = 7750.03 (s) ]\n",
            "09/08/2020 06:29:17 AM: [ train: Epoch = 10 | iter = 800/2012 | loss = 0.649 | batch P@1 = 77.36 % | elapsed time = 7785.58 (s) ]\n",
            "09/08/2020 06:29:53 AM: [ train: Epoch = 10 | iter = 900/2012 | loss = 0.662 | batch P@1 = 77.31 % | elapsed time = 7821.59 (s) ]\n",
            "09/08/2020 06:30:28 AM: [ train: Epoch = 10 | iter = 1000/2012 | loss = 0.698 | batch P@1 = 77.25 % | elapsed time = 7856.53 (s) ]\n",
            "09/08/2020 06:31:03 AM: [ train: Epoch = 10 | iter = 1100/2012 | loss = 0.741 | batch P@1 = 76.98 % | elapsed time = 7892.18 (s) ]\n",
            "09/08/2020 06:31:37 AM: [ train: Epoch = 10 | iter = 1200/2012 | loss = 0.691 | batch P@1 = 76.88 % | elapsed time = 7926.06 (s) ]\n",
            "09/08/2020 06:32:13 AM: [ train: Epoch = 10 | iter = 1300/2012 | loss = 0.672 | batch P@1 = 76.95 % | elapsed time = 7961.94 (s) ]\n",
            "09/08/2020 06:32:49 AM: [ train: Epoch = 10 | iter = 1400/2012 | loss = 0.724 | batch P@1 = 76.85 % | elapsed time = 7997.57 (s) ]\n",
            "09/08/2020 06:33:25 AM: [ train: Epoch = 10 | iter = 1500/2012 | loss = 0.693 | batch P@1 = 76.87 % | elapsed time = 8033.58 (s) ]\n",
            "09/08/2020 06:34:01 AM: [ train: Epoch = 10 | iter = 1600/2012 | loss = 0.749 | batch P@1 = 76.76 % | elapsed time = 8069.84 (s) ]\n",
            "09/08/2020 06:34:37 AM: [ train: Epoch = 10 | iter = 1700/2012 | loss = 0.660 | batch P@1 = 76.83 % | elapsed time = 8105.61 (s) ]\n",
            "09/08/2020 06:35:13 AM: [ train: Epoch = 10 | iter = 1800/2012 | loss = 0.746 | batch P@1 = 76.73 % | elapsed time = 8142.27 (s) ]\n",
            "09/08/2020 06:35:48 AM: [ train: Epoch = 10 | iter = 1900/2012 | loss = 0.674 | batch P@1 = 76.80 % | elapsed time = 8176.72 (s) ]\n",
            "09/08/2020 06:36:23 AM: [ train: Epoch = 10 | iter = 2000/2012 | loss = 0.682 | batch P@1 = 76.84 % | elapsed time = 8211.73 (s) ]\n",
            "09/08/2020 06:36:27 AM: [ train: Epoch = 10 | iter = 2012/2012 | loss = 0.690 | batch P@1 = 76.83 % | elapsed time = 8215.94 (s) ]\n",
            "09/08/2020 06:36:27 AM: [ train: Epoch 10 done. Time for epoch = 714.96 (s) ]\n",
            "09/08/2020 06:36:56 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 06:36:56 AM: [ Valid (shuffled): Epoch = 10 | avg loss = 1.184 | batch P@1 = 65.74 % | P@1,100 = 43.02% | P@3,100 = 63.40% | P@10,100 = 83.30% | valid time = 29.12 (s) ]\n",
            "09/08/2020 06:37:31 AM: [ train: Epoch = 11 | iter = 100/2012 | loss = 0.616 | batch P@1 = 79.70 % | elapsed time = 8279.79 (s) ]\n",
            "09/08/2020 06:38:07 AM: [ train: Epoch = 11 | iter = 200/2012 | loss = 0.638 | batch P@1 = 79.10 % | elapsed time = 8315.36 (s) ]\n",
            "09/08/2020 06:38:43 AM: [ train: Epoch = 11 | iter = 300/2012 | loss = 0.619 | batch P@1 = 79.07 % | elapsed time = 8351.35 (s) ]\n",
            "09/08/2020 06:39:18 AM: [ train: Epoch = 11 | iter = 400/2012 | loss = 0.671 | batch P@1 = 78.73 % | elapsed time = 8386.86 (s) ]\n",
            "09/08/2020 06:39:53 AM: [ train: Epoch = 11 | iter = 500/2012 | loss = 0.670 | batch P@1 = 78.51 % | elapsed time = 8422.16 (s) ]\n",
            "09/08/2020 06:40:28 AM: [ train: Epoch = 11 | iter = 600/2012 | loss = 0.644 | batch P@1 = 78.09 % | elapsed time = 8457.00 (s) ]\n",
            "09/08/2020 06:41:04 AM: [ train: Epoch = 11 | iter = 700/2012 | loss = 0.642 | batch P@1 = 78.19 % | elapsed time = 8492.49 (s) ]\n",
            "09/08/2020 06:41:40 AM: [ train: Epoch = 11 | iter = 800/2012 | loss = 0.658 | batch P@1 = 78.12 % | elapsed time = 8528.96 (s) ]\n",
            "09/08/2020 06:42:16 AM: [ train: Epoch = 11 | iter = 900/2012 | loss = 0.638 | batch P@1 = 78.20 % | elapsed time = 8564.53 (s) ]\n",
            "09/08/2020 06:42:52 AM: [ train: Epoch = 11 | iter = 1000/2012 | loss = 0.650 | batch P@1 = 78.24 % | elapsed time = 8600.50 (s) ]\n",
            "09/08/2020 06:43:28 AM: [ train: Epoch = 11 | iter = 1100/2012 | loss = 0.680 | batch P@1 = 78.12 % | elapsed time = 8636.35 (s) ]\n",
            "09/08/2020 06:44:05 AM: [ train: Epoch = 11 | iter = 1200/2012 | loss = 0.647 | batch P@1 = 78.15 % | elapsed time = 8673.43 (s) ]\n",
            "09/08/2020 06:44:39 AM: [ train: Epoch = 11 | iter = 1300/2012 | loss = 0.648 | batch P@1 = 78.16 % | elapsed time = 8707.92 (s) ]\n",
            "09/08/2020 06:45:14 AM: [ train: Epoch = 11 | iter = 1400/2012 | loss = 0.636 | batch P@1 = 78.21 % | elapsed time = 8742.71 (s) ]\n",
            "09/08/2020 06:45:50 AM: [ train: Epoch = 11 | iter = 1500/2012 | loss = 0.687 | batch P@1 = 78.15 % | elapsed time = 8778.88 (s) ]\n",
            "09/08/2020 06:46:26 AM: [ train: Epoch = 11 | iter = 1600/2012 | loss = 0.619 | batch P@1 = 78.21 % | elapsed time = 8814.69 (s) ]\n",
            "09/08/2020 06:47:01 AM: [ train: Epoch = 11 | iter = 1700/2012 | loss = 0.640 | batch P@1 = 78.22 % | elapsed time = 8849.97 (s) ]\n",
            "09/08/2020 06:47:36 AM: [ train: Epoch = 11 | iter = 1800/2012 | loss = 0.633 | batch P@1 = 78.27 % | elapsed time = 8885.17 (s) ]\n",
            "09/08/2020 06:48:12 AM: [ train: Epoch = 11 | iter = 1900/2012 | loss = 0.653 | batch P@1 = 78.23 % | elapsed time = 8921.18 (s) ]\n",
            "09/08/2020 06:48:48 AM: [ train: Epoch = 11 | iter = 2000/2012 | loss = 0.649 | batch P@1 = 78.20 % | elapsed time = 8956.84 (s) ]\n",
            "09/08/2020 06:48:52 AM: [ train: Epoch = 11 | iter = 2012/2012 | loss = 0.696 | batch P@1 = 78.20 % | elapsed time = 8961.19 (s) ]\n",
            "09/08/2020 06:48:52 AM: [ train: Epoch 11 done. Time for epoch = 716.11 (s) ]\n",
            "09/08/2020 06:49:22 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 06:49:22 AM: [ Valid (shuffled): Epoch = 11 | avg loss = 1.197 | batch P@1 = 65.35 % | P@1,100 = 42.96% | P@3,100 = 63.61% | P@10,100 = 83.79% | valid time = 29.49 (s) ]\n",
            "09/08/2020 06:49:56 AM: [ train: Epoch = 12 | iter = 100/2012 | loss = 0.574 | batch P@1 = 81.60 % | elapsed time = 9025.01 (s) ]\n",
            "09/08/2020 06:50:32 AM: [ train: Epoch = 12 | iter = 200/2012 | loss = 0.590 | batch P@1 = 80.20 % | elapsed time = 9060.50 (s) ]\n",
            "09/08/2020 06:51:08 AM: [ train: Epoch = 12 | iter = 300/2012 | loss = 0.607 | batch P@1 = 79.82 % | elapsed time = 9096.88 (s) ]\n",
            "09/08/2020 06:51:44 AM: [ train: Epoch = 12 | iter = 400/2012 | loss = 0.570 | batch P@1 = 80.18 % | elapsed time = 9132.36 (s) ]\n",
            "09/08/2020 06:52:20 AM: [ train: Epoch = 12 | iter = 500/2012 | loss = 0.600 | batch P@1 = 80.10 % | elapsed time = 9168.44 (s) ]\n",
            "09/08/2020 06:52:55 AM: [ train: Epoch = 12 | iter = 600/2012 | loss = 0.560 | batch P@1 = 80.25 % | elapsed time = 9204.29 (s) ]\n",
            "09/08/2020 06:53:32 AM: [ train: Epoch = 12 | iter = 700/2012 | loss = 0.586 | batch P@1 = 80.11 % | elapsed time = 9241.18 (s) ]\n",
            "09/08/2020 06:54:07 AM: [ train: Epoch = 12 | iter = 800/2012 | loss = 0.641 | batch P@1 = 79.92 % | elapsed time = 9275.78 (s) ]\n",
            "09/08/2020 06:54:42 AM: [ train: Epoch = 12 | iter = 900/2012 | loss = 0.608 | batch P@1 = 79.72 % | elapsed time = 9311.00 (s) ]\n",
            "09/08/2020 06:55:17 AM: [ train: Epoch = 12 | iter = 1000/2012 | loss = 0.574 | batch P@1 = 79.82 % | elapsed time = 9345.91 (s) ]\n",
            "09/08/2020 06:55:53 AM: [ train: Epoch = 12 | iter = 1100/2012 | loss = 0.641 | batch P@1 = 79.72 % | elapsed time = 9381.84 (s) ]\n",
            "09/08/2020 06:56:29 AM: [ train: Epoch = 12 | iter = 1200/2012 | loss = 0.601 | batch P@1 = 79.70 % | elapsed time = 9417.45 (s) ]\n",
            "09/08/2020 06:57:06 AM: [ train: Epoch = 12 | iter = 1300/2012 | loss = 0.592 | batch P@1 = 79.70 % | elapsed time = 9454.30 (s) ]\n",
            "09/08/2020 06:57:42 AM: [ train: Epoch = 12 | iter = 1400/2012 | loss = 0.615 | batch P@1 = 79.76 % | elapsed time = 9490.49 (s) ]\n",
            "09/08/2020 06:58:17 AM: [ train: Epoch = 12 | iter = 1500/2012 | loss = 0.625 | batch P@1 = 79.71 % | elapsed time = 9526.27 (s) ]\n",
            "09/08/2020 06:58:53 AM: [ train: Epoch = 12 | iter = 1600/2012 | loss = 0.603 | batch P@1 = 79.66 % | elapsed time = 9561.78 (s) ]\n",
            "09/08/2020 06:59:28 AM: [ train: Epoch = 12 | iter = 1700/2012 | loss = 0.610 | batch P@1 = 79.60 % | elapsed time = 9597.26 (s) ]\n",
            "09/08/2020 07:00:03 AM: [ train: Epoch = 12 | iter = 1800/2012 | loss = 0.635 | batch P@1 = 79.53 % | elapsed time = 9631.78 (s) ]\n",
            "09/08/2020 07:00:38 AM: [ train: Epoch = 12 | iter = 1900/2012 | loss = 0.612 | batch P@1 = 79.47 % | elapsed time = 9666.35 (s) ]\n",
            "09/08/2020 07:01:15 AM: [ train: Epoch = 12 | iter = 2000/2012 | loss = 0.627 | batch P@1 = 79.38 % | elapsed time = 9703.64 (s) ]\n",
            "09/08/2020 07:01:19 AM: [ train: Epoch = 12 | iter = 2012/2012 | loss = 0.633 | batch P@1 = 79.37 % | elapsed time = 9708.05 (s) ]\n",
            "09/08/2020 07:01:19 AM: [ train: Epoch 12 done. Time for epoch = 717.36 (s) ]\n",
            "09/08/2020 07:01:49 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 07:01:49 AM: [ Valid (shuffled): Epoch = 12 | avg loss = 1.201 | batch P@1 = 66.36 % | P@1,100 = 43.56% | P@3,100 = 63.74% | P@10,100 = 83.44% | valid time = 29.40 (s) ]\n",
            "09/08/2020 07:02:24 AM: [ train: Epoch = 13 | iter = 100/2012 | loss = 0.544 | batch P@1 = 81.00 % | elapsed time = 9772.40 (s) ]\n",
            "09/08/2020 07:02:59 AM: [ train: Epoch = 13 | iter = 200/2012 | loss = 0.598 | batch P@1 = 80.28 % | elapsed time = 9807.43 (s) ]\n",
            "09/08/2020 07:03:33 AM: [ train: Epoch = 13 | iter = 300/2012 | loss = 0.525 | batch P@1 = 80.80 % | elapsed time = 9841.38 (s) ]\n",
            "09/08/2020 07:04:09 AM: [ train: Epoch = 13 | iter = 400/2012 | loss = 0.566 | batch P@1 = 80.69 % | elapsed time = 9877.39 (s) ]\n",
            "09/08/2020 07:04:44 AM: [ train: Epoch = 13 | iter = 500/2012 | loss = 0.560 | batch P@1 = 80.66 % | elapsed time = 9912.81 (s) ]\n",
            "09/08/2020 07:05:18 AM: [ train: Epoch = 13 | iter = 600/2012 | loss = 0.557 | batch P@1 = 80.85 % | elapsed time = 9947.02 (s) ]\n",
            "09/08/2020 07:05:54 AM: [ train: Epoch = 13 | iter = 700/2012 | loss = 0.560 | batch P@1 = 80.91 % | elapsed time = 9983.01 (s) ]\n",
            "09/08/2020 07:06:30 AM: [ train: Epoch = 13 | iter = 800/2012 | loss = 0.579 | batch P@1 = 80.86 % | elapsed time = 10019.25 (s) ]\n",
            "09/08/2020 07:07:06 AM: [ train: Epoch = 13 | iter = 900/2012 | loss = 0.583 | batch P@1 = 80.76 % | elapsed time = 10055.29 (s) ]\n",
            "09/08/2020 07:07:42 AM: [ train: Epoch = 13 | iter = 1000/2012 | loss = 0.571 | batch P@1 = 80.85 % | elapsed time = 10090.72 (s) ]\n",
            "09/08/2020 07:08:19 AM: [ train: Epoch = 13 | iter = 1100/2012 | loss = 0.558 | batch P@1 = 80.86 % | elapsed time = 10127.60 (s) ]\n",
            "09/08/2020 07:08:54 AM: [ train: Epoch = 13 | iter = 1200/2012 | loss = 0.559 | batch P@1 = 80.96 % | elapsed time = 10162.82 (s) ]\n",
            "09/08/2020 07:09:30 AM: [ train: Epoch = 13 | iter = 1300/2012 | loss = 0.586 | batch P@1 = 80.93 % | elapsed time = 10199.28 (s) ]\n",
            "09/08/2020 07:10:06 AM: [ train: Epoch = 13 | iter = 1400/2012 | loss = 0.576 | batch P@1 = 80.91 % | elapsed time = 10234.43 (s) ]\n",
            "09/08/2020 07:10:41 AM: [ train: Epoch = 13 | iter = 1500/2012 | loss = 0.569 | batch P@1 = 80.90 % | elapsed time = 10270.22 (s) ]\n",
            "09/08/2020 07:11:16 AM: [ train: Epoch = 13 | iter = 1600/2012 | loss = 0.609 | batch P@1 = 80.78 % | elapsed time = 10304.57 (s) ]\n",
            "09/08/2020 07:11:52 AM: [ train: Epoch = 13 | iter = 1700/2012 | loss = 0.578 | batch P@1 = 80.74 % | elapsed time = 10340.44 (s) ]\n",
            "09/08/2020 07:12:28 AM: [ train: Epoch = 13 | iter = 1800/2012 | loss = 0.534 | batch P@1 = 80.79 % | elapsed time = 10377.15 (s) ]\n",
            "09/08/2020 07:13:03 AM: [ train: Epoch = 13 | iter = 1900/2012 | loss = 0.564 | batch P@1 = 80.82 % | elapsed time = 10411.99 (s) ]\n",
            "09/08/2020 07:13:39 AM: [ train: Epoch = 13 | iter = 2000/2012 | loss = 0.566 | batch P@1 = 80.82 % | elapsed time = 10447.95 (s) ]\n",
            "09/08/2020 07:13:43 AM: [ train: Epoch = 13 | iter = 2012/2012 | loss = 0.558 | batch P@1 = 80.83 % | elapsed time = 10452.22 (s) ]\n",
            "09/08/2020 07:13:43 AM: [ train: Epoch 13 done. Time for epoch = 714.76 (s) ]\n",
            "09/08/2020 07:14:13 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 07:14:13 AM: [ Valid (shuffled): Epoch = 13 | avg loss = 1.201 | batch P@1 = 66.40 % | P@1,100 = 42.37% | P@3,100 = 63.46% | P@10,100 = 84.05% | valid time = 29.37 (s) ]\n",
            "09/08/2020 07:14:49 AM: [ train: Epoch = 14 | iter = 100/2012 | loss = 0.512 | batch P@1 = 82.00 % | elapsed time = 10518.20 (s) ]\n",
            "09/08/2020 07:15:25 AM: [ train: Epoch = 14 | iter = 200/2012 | loss = 0.554 | batch P@1 = 81.38 % | elapsed time = 10553.67 (s) ]\n",
            "09/08/2020 07:16:00 AM: [ train: Epoch = 14 | iter = 300/2012 | loss = 0.519 | batch P@1 = 81.47 % | elapsed time = 10589.03 (s) ]\n",
            "09/08/2020 07:16:36 AM: [ train: Epoch = 14 | iter = 400/2012 | loss = 0.533 | batch P@1 = 81.45 % | elapsed time = 10624.61 (s) ]\n",
            "09/08/2020 07:17:11 AM: [ train: Epoch = 14 | iter = 500/2012 | loss = 0.525 | batch P@1 = 81.66 % | elapsed time = 10659.51 (s) ]\n",
            "09/08/2020 07:17:48 AM: [ train: Epoch = 14 | iter = 600/2012 | loss = 0.573 | batch P@1 = 81.38 % | elapsed time = 10696.91 (s) ]\n",
            "09/08/2020 07:18:25 AM: [ train: Epoch = 14 | iter = 700/2012 | loss = 0.528 | batch P@1 = 81.47 % | elapsed time = 10733.30 (s) ]\n",
            "09/08/2020 07:19:01 AM: [ train: Epoch = 14 | iter = 800/2012 | loss = 0.507 | batch P@1 = 81.54 % | elapsed time = 10769.79 (s) ]\n",
            "09/08/2020 07:19:35 AM: [ train: Epoch = 14 | iter = 900/2012 | loss = 0.573 | batch P@1 = 81.47 % | elapsed time = 10804.09 (s) ]\n",
            "09/08/2020 07:20:11 AM: [ train: Epoch = 14 | iter = 1000/2012 | loss = 0.553 | batch P@1 = 81.37 % | elapsed time = 10839.36 (s) ]\n",
            "09/08/2020 07:20:46 AM: [ train: Epoch = 14 | iter = 1100/2012 | loss = 0.526 | batch P@1 = 81.35 % | elapsed time = 10874.46 (s) ]\n",
            "09/08/2020 07:21:21 AM: [ train: Epoch = 14 | iter = 1200/2012 | loss = 0.560 | batch P@1 = 81.35 % | elapsed time = 10909.76 (s) ]\n",
            "09/08/2020 07:21:56 AM: [ train: Epoch = 14 | iter = 1300/2012 | loss = 0.490 | batch P@1 = 81.50 % | elapsed time = 10945.25 (s) ]\n",
            "09/08/2020 07:22:32 AM: [ train: Epoch = 14 | iter = 1400/2012 | loss = 0.537 | batch P@1 = 81.52 % | elapsed time = 10980.60 (s) ]\n",
            "09/08/2020 07:23:08 AM: [ train: Epoch = 14 | iter = 1500/2012 | loss = 0.525 | batch P@1 = 81.59 % | elapsed time = 11017.11 (s) ]\n",
            "09/08/2020 07:23:43 AM: [ train: Epoch = 14 | iter = 1600/2012 | loss = 0.565 | batch P@1 = 81.48 % | elapsed time = 11051.94 (s) ]\n",
            "09/08/2020 07:24:19 AM: [ train: Epoch = 14 | iter = 1700/2012 | loss = 0.529 | batch P@1 = 81.52 % | elapsed time = 11087.48 (s) ]\n",
            "09/08/2020 07:24:54 AM: [ train: Epoch = 14 | iter = 1800/2012 | loss = 0.539 | batch P@1 = 81.53 % | elapsed time = 11123.18 (s) ]\n",
            "09/08/2020 07:25:29 AM: [ train: Epoch = 14 | iter = 1900/2012 | loss = 0.545 | batch P@1 = 81.50 % | elapsed time = 11157.51 (s) ]\n",
            "09/08/2020 07:26:03 AM: [ train: Epoch = 14 | iter = 2000/2012 | loss = 0.520 | batch P@1 = 81.58 % | elapsed time = 11191.90 (s) ]\n",
            "09/08/2020 07:26:07 AM: [ train: Epoch = 14 | iter = 2012/2012 | loss = 0.593 | batch P@1 = 81.59 % | elapsed time = 11196.15 (s) ]\n",
            "09/08/2020 07:26:07 AM: [ train: Epoch 14 done. Time for epoch = 714.54 (s) ]\n",
            "09/08/2020 07:26:37 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 07:26:37 AM: [ Valid (shuffled): Epoch = 14 | avg loss = 1.242 | batch P@1 = 65.79 % | P@1,100 = 43.21% | P@3,100 = 63.33% | P@10,100 = 83.77% | valid time = 29.34 (s) ]\n",
            "09/08/2020 07:27:12 AM: [ train: Epoch = 15 | iter = 100/2012 | loss = 0.509 | batch P@1 = 82.05 % | elapsed time = 11260.91 (s) ]\n",
            "09/08/2020 07:27:47 AM: [ train: Epoch = 15 | iter = 200/2012 | loss = 0.545 | batch P@1 = 81.78 % | elapsed time = 11295.95 (s) ]\n",
            "09/08/2020 07:28:22 AM: [ train: Epoch = 15 | iter = 300/2012 | loss = 0.462 | batch P@1 = 82.50 % | elapsed time = 11331.24 (s) ]\n",
            "09/08/2020 07:28:57 AM: [ train: Epoch = 15 | iter = 400/2012 | loss = 0.472 | batch P@1 = 82.81 % | elapsed time = 11365.96 (s) ]\n",
            "09/08/2020 07:29:33 AM: [ train: Epoch = 15 | iter = 500/2012 | loss = 0.491 | batch P@1 = 82.88 % | elapsed time = 11401.94 (s) ]\n",
            "09/08/2020 07:30:09 AM: [ train: Epoch = 15 | iter = 600/2012 | loss = 0.499 | batch P@1 = 82.89 % | elapsed time = 11438.07 (s) ]\n",
            "09/08/2020 07:30:44 AM: [ train: Epoch = 15 | iter = 700/2012 | loss = 0.487 | batch P@1 = 82.98 % | elapsed time = 11473.07 (s) ]\n",
            "09/08/2020 07:31:19 AM: [ train: Epoch = 15 | iter = 800/2012 | loss = 0.512 | batch P@1 = 82.84 % | elapsed time = 11507.41 (s) ]\n",
            "09/08/2020 07:31:55 AM: [ train: Epoch = 15 | iter = 900/2012 | loss = 0.504 | batch P@1 = 82.84 % | elapsed time = 11543.53 (s) ]\n",
            "09/08/2020 07:32:31 AM: [ train: Epoch = 15 | iter = 1000/2012 | loss = 0.514 | batch P@1 = 82.90 % | elapsed time = 11579.35 (s) ]\n",
            "09/08/2020 07:33:06 AM: [ train: Epoch = 15 | iter = 1100/2012 | loss = 0.510 | batch P@1 = 82.84 % | elapsed time = 11614.50 (s) ]\n",
            "09/08/2020 07:33:42 AM: [ train: Epoch = 15 | iter = 1200/2012 | loss = 0.483 | batch P@1 = 82.91 % | elapsed time = 11650.79 (s) ]\n",
            "09/08/2020 07:34:19 AM: [ train: Epoch = 15 | iter = 1300/2012 | loss = 0.497 | batch P@1 = 82.89 % | elapsed time = 11687.41 (s) ]\n",
            "09/08/2020 07:34:54 AM: [ train: Epoch = 15 | iter = 1400/2012 | loss = 0.474 | batch P@1 = 82.95 % | elapsed time = 11722.99 (s) ]\n",
            "09/08/2020 07:35:30 AM: [ train: Epoch = 15 | iter = 1500/2012 | loss = 0.560 | batch P@1 = 82.81 % | elapsed time = 11758.37 (s) ]\n",
            "09/08/2020 07:36:05 AM: [ train: Epoch = 15 | iter = 1600/2012 | loss = 0.506 | batch P@1 = 82.81 % | elapsed time = 11794.01 (s) ]\n",
            "09/08/2020 07:36:42 AM: [ train: Epoch = 15 | iter = 1700/2012 | loss = 0.523 | batch P@1 = 82.78 % | elapsed time = 11830.76 (s) ]\n",
            "09/08/2020 07:37:18 AM: [ train: Epoch = 15 | iter = 1800/2012 | loss = 0.530 | batch P@1 = 82.66 % | elapsed time = 11866.43 (s) ]\n",
            "09/08/2020 07:37:53 AM: [ train: Epoch = 15 | iter = 1900/2012 | loss = 0.493 | batch P@1 = 82.63 % | elapsed time = 11902.25 (s) ]\n",
            "09/08/2020 07:38:28 AM: [ train: Epoch = 15 | iter = 2000/2012 | loss = 0.516 | batch P@1 = 82.60 % | elapsed time = 11936.89 (s) ]\n",
            "09/08/2020 07:38:32 AM: [ train: Epoch = 15 | iter = 2012/2012 | loss = 0.462 | batch P@1 = 82.60 % | elapsed time = 11941.00 (s) ]\n",
            "09/08/2020 07:38:32 AM: [ train: Epoch 15 done. Time for epoch = 715.49 (s) ]\n",
            "09/08/2020 07:39:02 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 07:39:02 AM: [ Valid (shuffled): Epoch = 15 | avg loss = 1.238 | batch P@1 = 66.29 % | P@1,100 = 43.09% | P@3,100 = 64.19% | P@10,100 = 83.39% | valid time = 29.51 (s) ]\n",
            "09/08/2020 07:39:37 AM: [ train: Epoch = 16 | iter = 100/2012 | loss = 0.464 | batch P@1 = 83.95 % | elapsed time = 12006.19 (s) ]\n",
            "09/08/2020 07:40:13 AM: [ train: Epoch = 16 | iter = 200/2012 | loss = 0.445 | batch P@1 = 84.30 % | elapsed time = 12041.58 (s) ]\n",
            "09/08/2020 07:40:49 AM: [ train: Epoch = 16 | iter = 300/2012 | loss = 0.460 | batch P@1 = 84.20 % | elapsed time = 12078.00 (s) ]\n",
            "09/08/2020 07:41:25 AM: [ train: Epoch = 16 | iter = 400/2012 | loss = 0.423 | batch P@1 = 84.50 % | elapsed time = 12113.41 (s) ]\n",
            "09/08/2020 07:42:00 AM: [ train: Epoch = 16 | iter = 500/2012 | loss = 0.461 | batch P@1 = 84.34 % | elapsed time = 12148.35 (s) ]\n",
            "09/08/2020 07:42:35 AM: [ train: Epoch = 16 | iter = 600/2012 | loss = 0.440 | batch P@1 = 84.17 % | elapsed time = 12184.00 (s) ]\n",
            "09/08/2020 07:43:11 AM: [ train: Epoch = 16 | iter = 700/2012 | loss = 0.468 | batch P@1 = 84.07 % | elapsed time = 12219.82 (s) ]\n",
            "09/08/2020 07:43:46 AM: [ train: Epoch = 16 | iter = 800/2012 | loss = 0.453 | batch P@1 = 84.14 % | elapsed time = 12254.71 (s) ]\n",
            "09/08/2020 07:44:22 AM: [ train: Epoch = 16 | iter = 900/2012 | loss = 0.481 | batch P@1 = 84.04 % | elapsed time = 12290.69 (s) ]\n",
            "09/08/2020 07:44:57 AM: [ train: Epoch = 16 | iter = 1000/2012 | loss = 0.528 | batch P@1 = 83.90 % | elapsed time = 12326.20 (s) ]\n",
            "09/08/2020 07:45:33 AM: [ train: Epoch = 16 | iter = 1100/2012 | loss = 0.440 | batch P@1 = 83.95 % | elapsed time = 12361.71 (s) ]\n",
            "09/08/2020 07:46:08 AM: [ train: Epoch = 16 | iter = 1200/2012 | loss = 0.478 | batch P@1 = 83.89 % | elapsed time = 12396.62 (s) ]\n",
            "09/08/2020 07:46:44 AM: [ train: Epoch = 16 | iter = 1300/2012 | loss = 0.459 | batch P@1 = 83.93 % | elapsed time = 12432.64 (s) ]\n",
            "09/08/2020 07:47:20 AM: [ train: Epoch = 16 | iter = 1400/2012 | loss = 0.495 | batch P@1 = 83.88 % | elapsed time = 12468.94 (s) ]\n",
            "09/08/2020 07:47:58 AM: [ train: Epoch = 16 | iter = 1500/2012 | loss = 0.458 | batch P@1 = 83.89 % | elapsed time = 12506.96 (s) ]\n",
            "09/08/2020 07:48:33 AM: [ train: Epoch = 16 | iter = 1600/2012 | loss = 0.469 | batch P@1 = 83.87 % | elapsed time = 12541.95 (s) ]\n",
            "09/08/2020 07:49:07 AM: [ train: Epoch = 16 | iter = 1700/2012 | loss = 0.532 | batch P@1 = 83.73 % | elapsed time = 12576.10 (s) ]\n",
            "09/08/2020 07:49:42 AM: [ train: Epoch = 16 | iter = 1800/2012 | loss = 0.467 | batch P@1 = 83.75 % | elapsed time = 12610.72 (s) ]\n",
            "09/08/2020 07:50:17 AM: [ train: Epoch = 16 | iter = 1900/2012 | loss = 0.494 | batch P@1 = 83.73 % | elapsed time = 12645.58 (s) ]\n",
            "09/08/2020 07:50:52 AM: [ train: Epoch = 16 | iter = 2000/2012 | loss = 0.485 | batch P@1 = 83.72 % | elapsed time = 12680.88 (s) ]\n",
            "09/08/2020 07:50:57 AM: [ train: Epoch = 16 | iter = 2012/2012 | loss = 0.596 | batch P@1 = 83.69 % | elapsed time = 12685.51 (s) ]\n",
            "09/08/2020 07:50:57 AM: [ train: Epoch 16 done. Time for epoch = 714.99 (s) ]\n",
            "09/08/2020 07:51:26 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 07:51:26 AM: [ Valid (shuffled): Epoch = 16 | avg loss = 1.229 | batch P@1 = 66.23 % | P@1,100 = 43.53% | P@3,100 = 64.04% | P@10,100 = 83.68% | valid time = 29.47 (s) ]\n",
            "09/08/2020 07:52:01 AM: [ train: Epoch = 17 | iter = 100/2012 | loss = 0.449 | batch P@1 = 84.55 % | elapsed time = 12749.90 (s) ]\n",
            "09/08/2020 07:52:37 AM: [ train: Epoch = 17 | iter = 200/2012 | loss = 0.453 | batch P@1 = 84.70 % | elapsed time = 12785.72 (s) ]\n",
            "09/08/2020 07:53:13 AM: [ train: Epoch = 17 | iter = 300/2012 | loss = 0.444 | batch P@1 = 84.53 % | elapsed time = 12821.91 (s) ]\n",
            "09/08/2020 07:53:48 AM: [ train: Epoch = 17 | iter = 400/2012 | loss = 0.436 | batch P@1 = 84.66 % | elapsed time = 12857.10 (s) ]\n",
            "09/08/2020 07:54:25 AM: [ train: Epoch = 17 | iter = 500/2012 | loss = 0.438 | batch P@1 = 84.76 % | elapsed time = 12893.64 (s) ]\n",
            "09/08/2020 07:55:01 AM: [ train: Epoch = 17 | iter = 600/2012 | loss = 0.429 | batch P@1 = 84.92 % | elapsed time = 12929.70 (s) ]\n",
            "09/08/2020 07:55:36 AM: [ train: Epoch = 17 | iter = 700/2012 | loss = 0.443 | batch P@1 = 84.84 % | elapsed time = 12965.11 (s) ]\n",
            "09/08/2020 07:56:12 AM: [ train: Epoch = 17 | iter = 800/2012 | loss = 0.439 | batch P@1 = 84.83 % | elapsed time = 13000.87 (s) ]\n",
            "09/08/2020 07:56:47 AM: [ train: Epoch = 17 | iter = 900/2012 | loss = 0.428 | batch P@1 = 84.83 % | elapsed time = 13036.06 (s) ]\n",
            "09/08/2020 07:57:23 AM: [ train: Epoch = 17 | iter = 1000/2012 | loss = 0.482 | batch P@1 = 84.67 % | elapsed time = 13071.54 (s) ]\n",
            "09/08/2020 07:57:58 AM: [ train: Epoch = 17 | iter = 1100/2012 | loss = 0.474 | batch P@1 = 84.63 % | elapsed time = 13106.78 (s) ]\n",
            "09/08/2020 07:58:34 AM: [ train: Epoch = 17 | iter = 1200/2012 | loss = 0.470 | batch P@1 = 84.57 % | elapsed time = 13143.02 (s) ]\n",
            "09/08/2020 07:59:10 AM: [ train: Epoch = 17 | iter = 1300/2012 | loss = 0.467 | batch P@1 = 84.55 % | elapsed time = 13178.64 (s) ]\n",
            "09/08/2020 07:59:45 AM: [ train: Epoch = 17 | iter = 1400/2012 | loss = 0.471 | batch P@1 = 84.52 % | elapsed time = 13214.10 (s) ]\n",
            "09/08/2020 08:00:20 AM: [ train: Epoch = 17 | iter = 1500/2012 | loss = 0.468 | batch P@1 = 84.51 % | elapsed time = 13248.71 (s) ]\n",
            "09/08/2020 08:00:56 AM: [ train: Epoch = 17 | iter = 1600/2012 | loss = 0.473 | batch P@1 = 84.43 % | elapsed time = 13284.44 (s) ]\n",
            "09/08/2020 08:01:30 AM: [ train: Epoch = 17 | iter = 1700/2012 | loss = 0.473 | batch P@1 = 84.40 % | elapsed time = 13319.13 (s) ]\n",
            "09/08/2020 08:02:07 AM: [ train: Epoch = 17 | iter = 1800/2012 | loss = 0.447 | batch P@1 = 84.43 % | elapsed time = 13355.37 (s) ]\n",
            "09/08/2020 08:02:42 AM: [ train: Epoch = 17 | iter = 1900/2012 | loss = 0.466 | batch P@1 = 84.39 % | elapsed time = 13390.69 (s) ]\n",
            "09/08/2020 08:03:18 AM: [ train: Epoch = 17 | iter = 2000/2012 | loss = 0.426 | batch P@1 = 84.45 % | elapsed time = 13426.68 (s) ]\n",
            "09/08/2020 08:03:22 AM: [ train: Epoch = 17 | iter = 2012/2012 | loss = 0.395 | batch P@1 = 84.46 % | elapsed time = 13430.90 (s) ]\n",
            "09/08/2020 08:03:22 AM: [ train: Epoch 17 done. Time for epoch = 715.89 (s) ]\n",
            "09/08/2020 08:03:52 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 08:03:52 AM: [ Valid (shuffled): Epoch = 17 | avg loss = 1.298 | batch P@1 = 66.21 % | P@1,100 = 43.93% | P@3,100 = 64.09% | P@10,100 = 83.46% | valid time = 29.62 (s) ]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3aJxoYD36aC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy40YBzPwsRX",
        "colab_type": "text"
      },
      "source": [
        "# Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp-o1f53AWyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "class TextData(data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        splitname,\n",
        "        maxlen=256, # max number of tokens per sentence # chua su dung\n",
        "        history_len=3,\n",
        "    ):\n",
        "        \n",
        "        df = open(f\"{splitname}.csv\", \"r\", encoding=\"utf-8\").readlines()\n",
        "         \n",
        "        self.max_hist_len = history_len\n",
        "        self.data = []\n",
        "        self.ids = []\n",
        "        history = []\n",
        "        for i in range(1, len(df)):\n",
        "            cparts = df[i - 1].strip().split(\",\")\n",
        "            sparts = df[i].strip().split(\",\")\n",
        "            if cparts[0] == sparts[0]:\n",
        "                \n",
        "                history.append(cparts[5].replace(\"_comma_\", \",\"))\n",
        "                idx = int(sparts[1])\n",
        "                if ((idx % 2) == 0):\n",
        "                    sentence1 = \" <SOC> \".join(history[-self.max_hist_len :]) # SOC start of comment\n",
        "                    sentence2 =  (sparts[5].replace(\"_comma_\", \",\"))\n",
        "                    \n",
        "                    self.data.append((sentence1, sentence2, sparts[2]))\n",
        "                    self.ids.append((sparts[0], sparts[1]))\n",
        "                    \n",
        "            else:\n",
        "                history = []\n",
        "                \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def getid(self, index):\n",
        "        return self.ids[index]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfp7L7DjAca4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_cands = TextData(\"ED/train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I34f5LtNAdAu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eb2f47bb-0d46-4a75-e755-56f83e394427"
      },
      "source": [
        "net = Net(device, False)\n",
        "\n",
        "if device == \"cuda\":\n",
        "  torch.cuda.set_device(-1) # get the lastest device (GPU)\n",
        "  net = torch.nn.DataParallel(net)\n",
        "  net.cuda()\n",
        "  \n",
        "# ctx_net.to(device)\n",
        "net.load_state_dict(torch.load(option.model_file), strict = False)\n",
        "net.eval()\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): Net(\n",
              "    (roberta): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (6): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (7): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (8): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (9): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (10): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (11): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): BertPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "    (fc): Linear(in_features=768, out_features=300, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDtxPKPQws-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter = torch.load(\"torch_pre_load/bert_train.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hE17FxrBzCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KURPVKBrAmFp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e67a0032-eee0-4ab4-aee4-a732e9bbe821"
      },
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "net.eval()\n",
        "all_cands = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, ex in enumerate(train_iter):\n",
        "      batch_size = ex[0].size(0)\n",
        "      params = [\n",
        "          field\n",
        "          if field is not None\n",
        "          else None\n",
        "          for field in ex\n",
        "      ]\n",
        "\n",
        "      cands = net(params[1][:,0,:])\n",
        "      all_cands.append(cands)\n",
        "\n",
        "  all_cands = torch.cat(all_cands, dim = 0)\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"Total time load candidates: \", end_time - start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total time load candidates:  58.19445323944092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKr3ZsEz8Tq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(all_cands, \"torch_pre_load/bert_all_cands.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvgLqb-bBHJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_cands = torch.load(\"torch_pre_load/bert_all_cands.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL9hOIleAoy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "def predict(context, top_n=5, normalize=False):\n",
        "    \"\"\"\n",
        "    returns a list of top_n tuples (\"sentence\", \"score\")\n",
        "    \"\"\"\n",
        "    context = txt2vec(context, max_tokens_length)\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        if device == \"cuda\":\n",
        "            context = context.cuda(non_blocking=True)\n",
        "       \n",
        "        ctx = net(context)\n",
        "        scores, index = score_candidates(ctx, all_cands, top_n, normalize)\n",
        "        response = []\n",
        "        for i, (score, index) in enumerate(zip(scores.squeeze(0), index.squeeze(0)), 1):\n",
        "            response.append((text_cands[index][1], float(score)))\n",
        "     \n",
        "        return response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX-AKolAAqDX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "ff069d61-be9d-47bf-bcab-ca453c70d9f3"
      },
      "source": [
        "outs = predict(\"I am totally out of money \", 10)\n",
        "for item in outs:\n",
        "    print(\"Score: \", item[1], \"\\nResponse: \", item[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score:  19.94879913330078 \n",
            "Response:  Awesome! So you'd recommend it if I were looking for a new phone?\n",
            "Score:  19.527507781982422 \n",
            "Response:  How long have you been in school?\n",
            "Score:  19.465566635131836 \n",
            "Response:  What did you study?\n",
            "Score:  19.106966018676758 \n",
            "Response:  Was there a prize you wanted from it? Or just recognition?\n",
            "Score:  18.963335037231445 \n",
            "Response:  You brought yourself. I am a glass half full type of lady lol\n",
            "Score:  18.95335578918457 \n",
            "Response:  Were you home when it started?\n",
            "Score:  18.83824920654297 \n",
            "Response:  I can understand that. Is it better for you this year?\n",
            "Score:  18.725860595703125 \n",
            "Response:  Why do you say that?\n",
            "Score:  18.608997344970703 \n",
            "Response:  wow how do you do it?\n",
            "Score:  18.585189819335938 \n",
            "Response:  I've never even heard of that before!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuhP_-3jDq8Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "eea10f18-e689-4e5e-c9d0-dd8d7b9430a3"
      },
      "source": [
        "index = 100\n",
        "print(\"Sentence: \", text_cands[index][0])\n",
        "print(\"Target: \", text_cands[index][1])\n",
        "print(10*\"*\")\n",
        "\n",
        "outs = predict(text_cands[index][0], 10)\n",
        "for item in outs:\n",
        "    print(\"Score: \", item[1], \"\\nResponse: \", item[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence:  You are never going to believe what I did <SOC> What did you do?  <SOC> Well, I normally do not feel comfortable lending things to my friends, but recently I mustered up the trust to loan my friend my vehicle.\n",
            "Target:  Ouch... Is it just for a day? Is your friend a safe driver?\n",
            "**********\n",
            "Score:  20.011789321899414 \n",
            "Response:  Money can always come back but your health is the most important thing.\n",
            "Score:  17.010778427124023 \n",
            "Response:  I bet! Sounds like it would be a big shock\n",
            "Score:  16.908485412597656 \n",
            "Response:  Oh, you don't work with them anymore? Or maybe it was because you decided to be a stay-at-home mother? \n",
            "Score:  16.868776321411133 \n",
            "Response:  What kind of tricks did he do\n",
            "Score:  16.69251823425293 \n",
            "Response:  That is disappointing. Did you confront them about it? I am sorry you have to deal with that.\n",
            "Score:  16.647926330566406 \n",
            "Response:  Oh man thats scary! What set it off?\n",
            "Score:  16.316850662231445 \n",
            "Response:  oh i hope she got better\n",
            "Score:  16.30805778503418 \n",
            "Response:  You are very thoughtful. I am sure he understands a chocolate craving is never to be denied!\n",
            "Score:  16.165380477905273 \n",
            "Response:  Was it a surprise?\n",
            "Score:  16.08214569091797 \n",
            "Response:  Come clean and take an F then\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_dusNBSwyQA",
        "colab_type": "text"
      },
      "source": [
        "# Dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUuXu1Hww18V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "02b6dae3-1d15-4aa6-a3bd-964d0c453455"
      },
      "source": [
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-07 08:53:21--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M  83.9MB/s    in 0.3s    \n",
            "\n",
            "2020-09-07 08:53:22 (83.9 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
            "\n",
            "--2020-09-07 08:53:22--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: ‘vi-vocab’\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2020-09-07 08:53:22 (5.66 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
            "\n",
            "--2020-09-07 08:53:22--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: ‘wordsegmenter.rdr’\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-09-07 08:53:22 (3.15 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-67_C73sxDeB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "c7ad5ab5-81ea-4cb8-c1e7-5c710fb6b0aa"
      },
      "source": [
        "!wget https://public.vinai.io/PhoBERT_base_transformers.tar.gz\n",
        "!tar -xzvf PhoBERT_base_transformers.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-07 08:55:21--  https://public.vinai.io/PhoBERT_base_transformers.tar.gz\n",
            "Resolving public.vinai.io (public.vinai.io)... 13.224.157.37, 13.224.157.83, 13.224.157.54, ...\n",
            "Connecting to public.vinai.io (public.vinai.io)|13.224.157.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 322405979 (307M) [application/x-tar]\n",
            "Saving to: ‘PhoBERT_base_transformers.tar.gz’\n",
            "\n",
            "PhoBERT_base_transf 100%[===================>] 307.47M  27.7MB/s    in 11s     \n",
            "\n",
            "2020-09-07 08:55:33 (27.7 MB/s) - ‘PhoBERT_base_transformers.tar.gz’ saved [322405979/322405979]\n",
            "\n",
            "PhoBERT_base_transformers/\n",
            "PhoBERT_base_transformers/config.json\n",
            "PhoBERT_base_transformers/bpe.codes\n",
            "PhoBERT_base_transformers/model.bin\n",
            "PhoBERT_base_transformers/dict.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJs2TvI8wtW_",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate for test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scUG6lsHwufV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Net(device, False)\n",
        "\n",
        "if device == \"cuda\":\n",
        "  torch.cuda.set_device(-1) # get the lastest device (GPU)\n",
        "  ctx_net = torch.nn.DataParallel(ctx_net)\n",
        "  ctx_net.cuda()\n",
        "  \n",
        "# ctx_net.to(device)\n",
        "ctx_net.load_state_dict(torch.load(option.model_file), strict = False)\n",
        "ctx_net.eval()\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwDFK9yXAwL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = EmpDataset(\n",
        "  \"ED/test\",\n",
        "  maxlen = max_tokens_length,\n",
        "  history_len = option.max_turn,\n",
        ")\n",
        "\n",
        "sh_test_iter = DataLoader(\n",
        "  dataset     = test_dataset,\n",
        "  batch_size  = option.batch_size,\n",
        "  shuffle     = True,\n",
        "  num_workers = 0,\n",
        "  collate_fn  = batchify,\n",
        "  pin_memory  = True,\n",
        ")\n",
        "\n",
        "un_test_iter = DataLoader(\n",
        "  dataset     = test_dataset,\n",
        "  batch_size  = option.batch_size,\n",
        "  shuffle     = False,\n",
        "  num_workers = 0,\n",
        "  collate_fn  = batchify,\n",
        "  pin_memory  = True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfiNHzEaAzhX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "186a2d91-cc1a-44a9-badd-cac3cf084193"
      },
      "source": [
        "validate(\n",
        "  0,\n",
        "  net,\n",
        "  sh_test_iter,\n",
        "  shuffle = True,\n",
        "  nb_candidates = option.hits_at_nb_cands\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "09/08/2020 08:16:50 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 08:16:50 AM: [ Valid (True): Epoch = 0 | avg loss = 1.191 | batch P@1 = 64.99 % | P@1,100 = 41.65% | P@3,100 = 61.98% | P@10,100 = 82.37% | valid time = 28.66 (s) ]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1908, device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qcIOE5GA0dI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "d4d16d32-a705-40e8-94a1-6e5ba0f61eea"
      },
      "source": [
        "validate(\n",
        "  0,\n",
        "  net,\n",
        "  un_test_iter,\n",
        "  shuffle = False, \n",
        "  nb_candidates= option.hits_at_nb_cands,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "09/08/2020 08:17:17 AM: [ Processing candidate top-K ]\n",
            "09/08/2020 08:17:17 AM: [ Valid (False): Epoch = 0 | avg loss = 2.349 | batch P@1 = 42.46 % | P@1,100 = 29.54% | P@3,100 = 57.19% | P@10,100 = 80.73% | valid time = 26.90 (s) ]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.3493, device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4jK6Fh4AU8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}